ucf101: 101 classes
Method : OURS
----AGE 1----
current_task  [98, 96, 18, 90, 75, 31, 95, 49, 43, 78]
current_head  61
Phase 2 : Train RGB Model in an Incremental Manner
=> base model: resnet50
Load the Previous Model
Copy the old Model
lambda_0  : [1.0, 0.022583179581272428]
Increment the Model
SplitCosineLinear(
  input_features=2048, output_features=183, sigma=tensor([3.8396]), eta=tensor([3.1335])
  (fc1): CosineLinear(input_features=2048, output_features=153, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=2048, output_features=30, sigma=1.0, eta=1.0)
)
video number : 897
video number + exemplar : 1152
DataLoader Constructed : Train 288
Optimizer Constructed
video number : 897
video number + exemplar : 897
Initialize Cosine Classifier
Computing the class mean vectors...
/home/ustc/anaconda3/envs/lhc/lib/python3.7/site-packages/torch/optim/sgd.py:101: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super(SGD, self).__init__(params, defaults)
/home/ustc/anaconda3/envs/lhc/lib/python3.7/site-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/ustc/anaconda3/envs/lhc/lib/python3.7/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/home/ustc/anaconda3/envs/lhc/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Traceback (most recent call last):
  File "main.py", line 102, in <module>
    main()
  File "main.py", line 72, in main
    train_i_cl.train_task(args, i, current_task, current_head, class_indexer, model_flow=model_flow, prefix=prefix)
  File "/home/ustc/ls/tcd_code/train/train_i_cl.py", line 529, in train_task
    _train(args, train_loader, model, criterion, optimizer, epoch, age, regularizer=regularizer, lambda_0=lambda_0, model_old=model_old, importance_list=importance_list)
  File "/home/ustc/ls/tcd_code/train/train_i_cl.py", line 196, in _train
    corr = Correlation.multilayer_correlation(old_feats, new_feats, stack_ids)
  File "/home/ustc/ls/tcd_code/utils/hsnet/model/base/correlation.py", line 33, in multilayer_correlation
    corr_l4 = torch.stack(corrs[-stack_ids[2]:-stack_ids[1]]).transpose(0, 1).contiguous()
RuntimeError: stack expects each tensor to be equal size, but got [32, 56, 56, 56, 56] at entry 0 and [32, 28, 28, 28, 28] at entry 3
tensor([ 3,  9, 13])
(12,)