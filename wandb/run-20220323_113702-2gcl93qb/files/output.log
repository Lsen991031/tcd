ucf101: 101 classes
Method : OURS
----AGE 22----
current_task  [61, 1]
current_head  95
Phase 2 : Train RGB Model in an Incremental Manner
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Load the Previous Model
Copy the old Model
lambda_0  : [1.0, 0.06819090848492927]
Increment the Model
SplitCosineLinear(
  input_features=512, output_features=285, sigma=tensor([3.9359]), eta=tensor([3.1189])
  (fc1): CosineLinear(input_features=512, output_features=279, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
video number : 189
video number + exemplar : 654
DataLoader Constructed : Train 20
Optimizer Constructed
video number : 189
video number + exemplar : 189
Initialize Cosine Classifier
Computing the class mean vectors...
/home/ustc/anaconda3/envs/lhc/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2022-03-23 11:37:29.819825
Epoch: [0][0/20], lr: 0.00100	Time 5.716 (5.716)	Data 2.277 (2.277)	Loss 0.0724 (0.0724)	Loss CE 0.0144 (0.0144)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5792 (0.5792)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9083], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1014], device='cuda:0', requires_grad=True)
2022-03-23 11:37:49.528733
Epoch: [1][0/20], lr: 0.00100	Time 3.396 (3.396)	Data 2.277 (2.277)	Loss 0.0892 (0.0892)	Loss CE 0.0302 (0.0302)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5898 (0.5898)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.8920], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.0918], device='cuda:0', requires_grad=True)
2022-03-23 11:38:08.047007
Epoch: [2][0/20], lr: 0.00100	Time 3.373 (3.373)	Data 2.163 (2.163)	Loss 0.1588 (0.1588)	Loss CE 0.1011 (0.1011)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5764 (0.5764)	Loss REG 0.0000 (0.0000)	Prec@1 96.875 (96.875)
Sigma : Parameter containing:
tensor([3.8878], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.0893], device='cuda:0', requires_grad=True)
2022-03-23 11:38:26.742696
Epoch: [3][0/20], lr: 0.00100	Time 3.335 (3.335)	Data 2.145 (2.145)	Loss 0.0651 (0.0651)	Loss CE 0.0063 (0.0063)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5877 (0.5877)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.8900], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.0903], device='cuda:0', requires_grad=True)
2022-03-23 11:38:44.499860
Epoch: [4][0/20], lr: 0.00100	Time 3.219 (3.219)	Data 2.384 (2.384)	Loss 0.0673 (0.0673)	Loss CE 0.0049 (0.0049)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6236 (0.6236)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.8945], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.0927], device='cuda:0', requires_grad=True)
2022-03-23 11:39:01.786738
Epoch: [5][0/20], lr: 0.00100	Time 2.993 (2.993)	Data 2.206 (2.206)	Loss 0.0664 (0.0664)	Loss CE 0.0051 (0.0051)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6117 (0.6117)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.8998], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.0956], device='cuda:0', requires_grad=True)
2022-03-23 11:39:19.247115
Epoch: [6][0/20], lr: 0.00100	Time 3.108 (3.108)	Data 2.208 (2.208)	Loss 0.0630 (0.0630)	Loss CE 0.0031 (0.0031)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5976 (0.5976)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9029], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.0976], device='cuda:0', requires_grad=True)
2022-03-23 11:39:36.783548
Epoch: [7][0/20], lr: 0.00100	Time 3.012 (3.012)	Data 1.761 (1.761)	Loss 0.0670 (0.0670)	Loss CE 0.0060 (0.0060)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6094 (0.6094)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9028], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.0978], device='cuda:0', requires_grad=True)
2022-03-23 11:39:54.422387
Epoch: [8][0/20], lr: 0.00100	Time 3.128 (3.128)	Data 1.903 (1.903)	Loss 0.0669 (0.0669)	Loss CE 0.0137 (0.0137)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5310 (0.5310)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9069], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1003], device='cuda:0', requires_grad=True)
2022-03-23 11:40:11.766671
Epoch: [9][0/20], lr: 0.00100	Time 3.254 (3.254)	Data 2.083 (2.083)	Loss 0.0610 (0.0610)	Loss CE 0.0021 (0.0021)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5887 (0.5887)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9126], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1037], device='cuda:0', requires_grad=True)
2022-03-23 11:40:28.863255
Epoch: [10][0/20], lr: 0.00100	Time 2.831 (2.831)	Data 1.666 (1.666)	Loss 0.0606 (0.0606)	Loss CE 0.0046 (0.0046)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5589 (0.5589)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9168], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1062], device='cuda:0', requires_grad=True)
2022-03-23 11:40:45.604600
Epoch: [11][0/20], lr: 0.00100	Time 3.082 (3.082)	Data 1.907 (1.907)	Loss 0.0667 (0.0667)	Loss CE 0.0074 (0.0074)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5928 (0.5928)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9205], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1084], device='cuda:0', requires_grad=True)
2022-03-23 11:41:05.082682
Epoch: [12][0/20], lr: 0.00100	Time 3.250 (3.250)	Data 2.318 (2.318)	Loss 0.0841 (0.0841)	Loss CE 0.0222 (0.0222)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6185 (0.6185)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9216], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1091], device='cuda:0', requires_grad=True)
2022-03-23 11:41:26.205759
Epoch: [13][0/20], lr: 0.00100	Time 3.260 (3.260)	Data 2.147 (2.147)	Loss 0.0663 (0.0663)	Loss CE 0.0037 (0.0037)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6246 (0.6246)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9215], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:41:47.734422
Epoch: [14][0/20], lr: 0.00100	Time 3.650 (3.650)	Data 2.759 (2.759)	Loss 0.0754 (0.0754)	Loss CE 0.0199 (0.0199)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5538 (0.5538)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9231], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1094], device='cuda:0', requires_grad=True)
2022-03-23 11:42:08.365970
Epoch: [15][0/20], lr: 0.00100	Time 3.272 (3.272)	Data 1.945 (1.945)	Loss 0.0643 (0.0643)	Loss CE 0.0026 (0.0026)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6156 (0.6156)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9229], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1091], device='cuda:0', requires_grad=True)
2022-03-23 11:42:30.271582
Epoch: [16][0/20], lr: 0.00100	Time 3.551 (3.551)	Data 2.391 (2.391)	Loss 0.0602 (0.0602)	Loss CE 0.0001 (0.0001)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5997 (0.5997)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9245], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1098], device='cuda:0', requires_grad=True)
2022-03-23 11:42:53.791148
Epoch: [17][0/20], lr: 0.00100	Time 3.609 (3.609)	Data 2.318 (2.318)	Loss 0.0636 (0.0636)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6338 (0.6338)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9254], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1101], device='cuda:0', requires_grad=True)
2022-03-23 11:43:17.165023
Epoch: [18][0/20], lr: 0.00100	Time 3.637 (3.637)	Data 2.583 (2.583)	Loss 0.0635 (0.0635)	Loss CE 0.0016 (0.0016)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6175 (0.6175)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9241], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1088], device='cuda:0', requires_grad=True)
2022-03-23 11:43:40.444180
Epoch: [19][0/20], lr: 0.00100	Time 3.490 (3.490)	Data 2.233 (2.233)	Loss 0.0575 (0.0575)	Loss CE 0.0003 (0.0003)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5714 (0.5714)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9243], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1086], device='cuda:0', requires_grad=True)
2022-03-23 11:44:02.290935
Epoch: [20][0/20], lr: 0.00010	Time 3.558 (3.558)	Data 2.265 (2.265)	Loss 0.0571 (0.0571)	Loss CE 0.0010 (0.0010)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5603 (0.5603)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9244], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:44:23.089476
Epoch: [21][0/20], lr: 0.00010	Time 3.155 (3.155)	Data 1.861 (1.861)	Loss 0.0577 (0.0577)	Loss CE 0.0003 (0.0003)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5734 (0.5734)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9246], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1088], device='cuda:0', requires_grad=True)
2022-03-23 11:44:43.904661
Epoch: [22][0/20], lr: 0.00010	Time 3.071 (3.071)	Data 2.329 (2.329)	Loss 0.0571 (0.0571)	Loss CE 0.0006 (0.0006)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5640 (0.5640)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9247], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1088], device='cuda:0', requires_grad=True)
2022-03-23 11:45:05.274979
Epoch: [23][0/20], lr: 0.00010	Time 3.759 (3.759)	Data 2.582 (2.582)	Loss 0.0593 (0.0593)	Loss CE 0.0005 (0.0005)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5866 (0.5866)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9243], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1086], device='cuda:0', requires_grad=True)
2022-03-23 11:45:28.574789
Epoch: [24][0/20], lr: 0.00010	Time 3.248 (3.248)	Data 1.878 (1.878)	Loss 0.0620 (0.0620)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6172 (0.6172)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9244], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1086], device='cuda:0', requires_grad=True)
2022-03-23 11:45:52.394343
Epoch: [25][0/20], lr: 0.00010	Time 3.818 (3.818)	Data 2.672 (2.672)	Loss 0.0750 (0.0750)	Loss CE 0.0165 (0.0165)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5845 (0.5845)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9244], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1085], device='cuda:0', requires_grad=True)
2022-03-23 11:46:15.705948
Epoch: [26][0/20], lr: 0.00010	Time 3.198 (3.198)	Data 1.739 (1.739)	Loss 0.0708 (0.0708)	Loss CE 0.0114 (0.0114)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5929 (0.5929)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9244], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1085], device='cuda:0', requires_grad=True)
2022-03-23 11:46:37.353108
Epoch: [27][0/20], lr: 0.00010	Time 3.329 (3.329)	Data 2.424 (2.424)	Loss 0.0609 (0.0609)	Loss CE 0.0007 (0.0007)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6007 (0.6007)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9246], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1086], device='cuda:0', requires_grad=True)
2022-03-23 11:46:58.420266
Epoch: [28][0/20], lr: 0.00010	Time 3.348 (3.348)	Data 2.224 (2.224)	Loss 0.0619 (0.0619)	Loss CE 0.0007 (0.0007)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6111 (0.6111)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9247], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1086], device='cuda:0', requires_grad=True)
2022-03-23 11:47:19.102810
Epoch: [29][0/20], lr: 0.00010	Time 3.200 (3.200)	Data 2.429 (2.429)	Loss 0.0643 (0.0643)	Loss CE 0.0003 (0.0003)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6387 (0.6387)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9248], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:47:40.856631
Epoch: [30][0/20], lr: 0.00001	Time 3.765 (3.765)	Data 2.247 (2.247)	Loss 0.0649 (0.0649)	Loss CE 0.0012 (0.0012)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6361 (0.6361)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9248], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:48:04.705984
Epoch: [31][0/20], lr: 0.00001	Time 3.677 (3.677)	Data 2.674 (2.674)	Loss 0.0607 (0.0607)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6040 (0.6040)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9248], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:48:28.349822
Epoch: [32][0/20], lr: 0.00001	Time 3.830 (3.830)	Data 2.699 (2.699)	Loss 0.0625 (0.0625)	Loss CE 0.0048 (0.0048)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5759 (0.5759)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9248], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:48:51.779528
Epoch: [33][0/20], lr: 0.00001	Time 3.244 (3.244)	Data 1.793 (1.793)	Loss 0.0597 (0.0597)	Loss CE 0.0005 (0.0005)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5908 (0.5908)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9248], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:49:13.612029
Epoch: [34][0/20], lr: 0.00001	Time 3.520 (3.520)	Data 2.427 (2.427)	Loss 0.0660 (0.0660)	Loss CE 0.0057 (0.0057)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6027 (0.6027)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9248], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:49:34.582349
Epoch: [35][0/20], lr: 0.00001	Time 3.255 (3.255)	Data 2.221 (2.221)	Loss 0.0585 (0.0585)	Loss CE 0.0029 (0.0029)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5543 (0.5543)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9248], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:49:55.104750
Epoch: [36][0/20], lr: 0.00001	Time 3.024 (3.024)	Data 1.774 (1.774)	Loss 0.0595 (0.0595)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5921 (0.5921)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9248], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:50:17.426699
Epoch: [37][0/20], lr: 0.00001	Time 3.708 (3.708)	Data 2.429 (2.429)	Loss 0.0643 (0.0643)	Loss CE 0.0011 (0.0011)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6318 (0.6318)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9248], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:50:41.325641
Epoch: [38][0/20], lr: 0.00001	Time 3.709 (3.709)	Data 2.386 (2.386)	Loss 0.0599 (0.0599)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5954 (0.5954)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9249], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:51:04.826667
Epoch: [39][0/20], lr: 0.00001	Time 3.580 (3.580)	Data 1.919 (1.919)	Loss 0.0656 (0.0656)	Loss CE 0.0031 (0.0031)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6241 (0.6241)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9249], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:51:28.338579
Epoch: [40][0/20], lr: 0.00001	Time 3.531 (3.531)	Data 2.422 (2.422)	Loss 0.0603 (0.0603)	Loss CE 0.0005 (0.0005)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5971 (0.5971)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9249], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:51:49.777079
Epoch: [41][0/20], lr: 0.00001	Time 3.290 (3.290)	Data 1.840 (1.840)	Loss 0.0576 (0.0576)	Loss CE 0.0004 (0.0004)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5709 (0.5709)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9249], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:52:11.593861
Epoch: [42][0/20], lr: 0.00001	Time 3.658 (3.658)	Data 2.521 (2.521)	Loss 0.0606 (0.0606)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6036 (0.6036)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9249], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:52:31.937992
Epoch: [43][0/20], lr: 0.00001	Time 3.311 (3.311)	Data 2.448 (2.448)	Loss 0.0642 (0.0642)	Loss CE 0.0016 (0.0016)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6249 (0.6249)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9249], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:52:54.376830
Epoch: [44][0/20], lr: 0.00001	Time 3.799 (3.799)	Data 2.590 (2.590)	Loss 0.0632 (0.0632)	Loss CE 0.0036 (0.0036)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5955 (0.5955)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9249], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:53:17.553902
Epoch: [45][0/20], lr: 0.00001	Time 3.653 (3.653)	Data 2.266 (2.266)	Loss 0.0629 (0.0629)	Loss CE 0.0040 (0.0040)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5881 (0.5881)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9249], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:53:41.030727
Epoch: [46][0/20], lr: 0.00001	Time 3.710 (3.710)	Data 2.279 (2.279)	Loss 0.0561 (0.0561)	Loss CE 0.0003 (0.0003)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5562 (0.5562)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9249], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:54:04.299097
Epoch: [47][0/20], lr: 0.00001	Time 3.633 (3.633)	Data 2.498 (2.498)	Loss 0.0599 (0.0599)	Loss CE 0.0004 (0.0004)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5946 (0.5946)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9249], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:54:24.619745
Epoch: [48][0/20], lr: 0.00001	Time 3.488 (3.488)	Data 2.100 (2.100)	Loss 0.0570 (0.0570)	Loss CE 0.0005 (0.0005)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5637 (0.5637)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9249], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:54:43.954482
Epoch: [49][0/20], lr: 0.00001	Time 3.043 (3.043)	Data 1.938 (1.938)	Loss 0.0707 (0.0707)	Loss CE 0.0050 (0.0050)	Loss KD (Logit) 0.0007 (0.0007)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6557 (0.6557)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9250], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
Update Importance Mask...
Parameter containing:
tensor([0.0948], device='cuda:0', requires_grad=True) Parameter containing:
tensor([0.1895], device='cuda:0', requires_grad=True) Parameter containing:
tensor([0.2845], device='cuda:0', requires_grad=True) Parameter containing:
tensor([0.3790], device='cuda:0', requires_grad=True) Phase 3 : Manage Exemplar Sets
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Construct Exemplar Set
Load the Model
SplitCosineLinear(
  input_features=512, output_features=285, sigma=tensor([3.9250]), eta=tensor([3.1087])
  (fc1): CosineLinear(input_features=512, output_features=279, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
Exemplar per class : 5
video number : 189
video number + exemplar : 189
Phase 4 : Class-balanced Fine-Tuning
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Load the Model
SplitCosineLinear(
  input_features=512, output_features=285, sigma=tensor([3.9250]), eta=tensor([3.1087])
  (fc1): CosineLinear(input_features=512, output_features=279, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
exemplar : 475
DataLoader CBF Constructed : Train 14
Optimizer Constructed
2022-03-23 11:55:20.747254
Epoch: [0][0/14], lr: 0.00050	Time 3.446 (3.446)	Data 2.550 (2.550)	Loss 0.0006 (0.0006)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9251], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:55:33.220007
Epoch: [1][0/14], lr: 0.00050	Time 3.407 (3.407)	Data 2.699 (2.699)	Loss 0.0034 (0.0034)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9258], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1091], device='cuda:0', requires_grad=True)
2022-03-23 11:55:45.792027
Epoch: [2][0/14], lr: 0.00050	Time 3.651 (3.651)	Data 2.738 (2.738)	Loss 0.0003 (0.0003)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9260], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1090], device='cuda:0', requires_grad=True)
2022-03-23 11:55:58.536541
Epoch: [3][0/14], lr: 0.00050	Time 3.753 (3.753)	Data 2.963 (2.963)	Loss 0.0004 (0.0004)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9260], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1087], device='cuda:0', requires_grad=True)
2022-03-23 11:56:11.031684
Epoch: [4][0/14], lr: 0.00050	Time 3.502 (3.502)	Data 2.703 (2.703)	Loss 0.0001 (0.0001)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9260], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1086], device='cuda:0', requires_grad=True)
2022-03-23 11:56:23.032611
Epoch: [5][0/14], lr: 0.00050	Time 3.262 (3.262)	Data 2.436 (2.436)	Loss 0.0005 (0.0005)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9259], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1083], device='cuda:0', requires_grad=True)
2022-03-23 11:56:35.025866
Epoch: [6][0/14], lr: 0.00050	Time 3.014 (3.014)	Data 2.150 (2.150)	Loss 0.0003 (0.0003)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9257], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1080], device='cuda:0', requires_grad=True)
2022-03-23 11:56:46.046838
Epoch: [7][0/14], lr: 0.00050	Time 3.219 (3.219)	Data 2.448 (2.448)	Loss 0.0005 (0.0005)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9257], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1077], device='cuda:0', requires_grad=True)
2022-03-23 11:56:56.762032
Epoch: [8][0/14], lr: 0.00050	Time 3.102 (3.102)	Data 2.376 (2.376)	Loss 0.0005 (0.0005)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9258], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1076], device='cuda:0', requires_grad=True)
2022-03-23 11:57:07.298508
Epoch: [9][0/14], lr: 0.00050	Time 3.043 (3.043)	Data 2.396 (2.396)	Loss 0.0313 (0.0313)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9259], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1073], device='cuda:0', requires_grad=True)
2022-03-23 11:57:18.006045
Epoch: [10][0/14], lr: 0.00050	Time 2.986 (2.986)	Data 2.392 (2.392)	Loss 0.0021 (0.0021)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9258], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1071], device='cuda:0', requires_grad=True)
2022-03-23 11:57:28.657619
Epoch: [11][0/14], lr: 0.00050	Time 3.133 (3.133)	Data 2.363 (2.363)	Loss 0.0001 (0.0001)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9257], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1068], device='cuda:0', requires_grad=True)
2022-03-23 11:57:38.902595
Epoch: [12][0/14], lr: 0.00050	Time 2.903 (2.903)	Data 2.288 (2.288)	Loss 0.0218 (0.0218)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9255], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1064], device='cuda:0', requires_grad=True)
2022-03-23 11:57:48.137391
Epoch: [13][0/14], lr: 0.00050	Time 3.045 (3.045)	Data 2.047 (2.047)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9247], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1057], device='cuda:0', requires_grad=True)
2022-03-23 11:57:58.515169
Epoch: [14][0/14], lr: 0.00050	Time 3.185 (3.185)	Data 2.421 (2.421)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9242], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1052], device='cuda:0', requires_grad=True)
2022-03-23 11:58:10.630229
Epoch: [15][0/14], lr: 0.00050	Time 3.233 (3.233)	Data 2.410 (2.410)	Loss 0.0005 (0.0005)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9240], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1049], device='cuda:0', requires_grad=True)
2022-03-23 11:58:22.826810
Epoch: [16][0/14], lr: 0.00050	Time 3.109 (3.109)	Data 2.061 (2.061)	Loss 0.0003 (0.0003)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9237], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1045], device='cuda:0', requires_grad=True)
2022-03-23 11:58:34.949038
Epoch: [17][0/14], lr: 0.00050	Time 3.214 (3.214)	Data 2.488 (2.488)	Loss 0.0005 (0.0005)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9233], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1041], device='cuda:0', requires_grad=True)
2022-03-23 11:58:47.442422
Epoch: [18][0/14], lr: 0.00050	Time 3.428 (3.428)	Data 2.697 (2.697)	Loss 0.0004 (0.0004)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9232], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1038], device='cuda:0', requires_grad=True)
2022-03-23 11:58:59.356532
Epoch: [19][0/14], lr: 0.00050	Time 2.875 (2.875)	Data 1.950 (1.950)	Loss 0.0001 (0.0001)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9231], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1036], device='cuda:0', requires_grad=True)
Phase 5 : Eval RGB Model for the Tasks Trained so far
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Load the Trained Model from checkpoint/ucf101/51/2/005/task_022.pth.tar
exemplar : 475
Computing the class mean vectors...
Eval Task 0 for Age 22
Current Task : [37, 97, 56, 55, 33, 84, 3, 4, 72, 59, 66, 48, 65, 91, 99, 39, 34, 22, 67, 74, 19, 35, 9, 86, 88, 63, 85, 38, 54, 25, 57, 62, 83, 76, 6, 13, 2, 53, 8, 24, 44, 12, 100, 29, 5, 17, 15, 73, 47, 27, 46]
video number : 1909
video number + exemplar : 1909
DataLoader Constructed
Test: [0/120]	Time 7.242 (7.242)	Prec@1 81.250 (81.250)
Test: [100/120]	Time 0.414 (0.555)	Prec@1 56.250 (58.601)
Testing Results: Prec@1 58.931
Classify using the NME Classifier...
Test (NME): [0/120]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Test (NME): [100/120]	Time 0.000 (0.000)	Prec@1 56.250 (65.408)
Testing Results (NME): Prec@1 66.003
Eval Task 1 for Age 22
Current Task : [98, 96]
video number : 70
video number + exemplar : 70
DataLoader Constructed
Test: [0/5]	Time 3.845 (3.845)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 54.286
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 62.857
Eval Task 2 for Age 22
Current Task : [18, 90]
video number : 56
video number + exemplar : 56
DataLoader Constructed
Test: [0/4]	Time 4.488 (4.488)	Prec@1 43.750 (43.750)
Testing Results: Prec@1 64.286
Classify using the NME Classifier...
Test (NME): [0/4]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 75.000
Eval Task 3 for Age 22
Current Task : [75, 31]
video number : 73
video number + exemplar : 73
DataLoader Constructed
Test: [0/5]	Time 5.164 (5.164)	Prec@1 56.250 (56.250)
Testing Results: Prec@1 63.014
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 62.500 (62.500)
Testing Results (NME): Prec@1 64.384
Eval Task 4 for Age 22
Current Task : [95, 49]
video number : 62
video number + exemplar : 62
DataLoader Constructed
Test: [0/4]	Time 4.206 (4.206)	Prec@1 81.250 (81.250)
Testing Results: Prec@1 85.484
Classify using the NME Classifier...
Test (NME): [0/4]	Time 0.000 (0.000)	Prec@1 93.750 (93.750)
Testing Results (NME): Prec@1 90.323
Eval Task 5 for Age 22
Current Task : [43, 78]
video number : 92
video number + exemplar : 92
DataLoader Constructed
Test: [0/6]	Time 4.153 (4.153)	Prec@1 56.250 (56.250)
Testing Results: Prec@1 61.957
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 69.565
Eval Task 6 for Age 22
Current Task : [23, 68]
video number : 84
video number + exemplar : 84
DataLoader Constructed
Test: [0/6]	Time 4.346 (4.346)	Prec@1 50.000 (50.000)
Testing Results: Prec@1 60.714
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 43.750 (43.750)
Testing Results (NME): Prec@1 48.810
Eval Task 7 for Age 22
Current Task : [16, 7]
video number : 84
video number + exemplar : 84
DataLoader Constructed
Test: [0/6]	Time 4.499 (4.499)	Prec@1 18.750 (18.750)
Testing Results: Prec@1 33.333
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 12.500 (12.500)
Testing Results (NME): Prec@1 30.952
Eval Task 8 for Age 22
Current Task : [26, 21]
video number : 84
video number + exemplar : 84
DataLoader Constructed
Test: [0/6]	Time 4.090 (4.090)	Prec@1 87.500 (87.500)
Testing Results: Prec@1 82.143
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 70.238
Eval Task 9 for Age 22
Current Task : [50, 70]
video number : 78
video number + exemplar : 78
DataLoader Constructed
Test: [0/5]	Time 3.884 (3.884)	Prec@1 81.250 (81.250)
Testing Results: Prec@1 80.769
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 74.359
Eval Task 10 for Age 22
Current Task : [32, 52]
video number : 72
video number + exemplar : 72
DataLoader Constructed
Test: [0/5]	Time 3.996 (3.996)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 56.944
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 66.667
Eval Task 11 for Age 22
Current Task : [11, 69]
video number : 68
video number + exemplar : 68
DataLoader Constructed
Test: [0/5]	Time 4.082 (4.082)	Prec@1 75.000 (75.000)
Testing Results: Prec@1 79.412
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 77.941
Eval Task 12 for Age 22
Current Task : [93, 14]
video number : 62
video number + exemplar : 62
DataLoader Constructed
Test: [0/4]	Time 3.678 (3.678)	Prec@1 31.250 (31.250)
Testing Results: Prec@1 51.613
Classify using the NME Classifier...
Test (NME): [0/4]	Time 0.000 (0.000)	Prec@1 43.750 (43.750)
Testing Results (NME): Prec@1 46.774
Eval Task 13 for Age 22
Current Task : [79, 10]
video number : 70
video number + exemplar : 70
DataLoader Constructed
Test: [0/5]	Time 4.295 (4.295)	Prec@1 93.750 (93.750)
Testing Results: Prec@1 85.714
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 81.429
Eval Task 14 for Age 22
Current Task : [80, 77]
video number : 83
video number + exemplar : 83
DataLoader Constructed
Test: [0/6]	Time 4.500 (4.500)	Prec@1 50.000 (50.000)
Testing Results: Prec@1 65.060
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 37.500 (37.500)
Testing Results (NME): Prec@1 61.446
Eval Task 15 for Age 22
Current Task : [81, 28]
video number : 68
video number + exemplar : 68
DataLoader Constructed
Test: [0/5]	Time 3.905 (3.905)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 76.471
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 69.118
Eval Task 16 for Age 22
Current Task : [82, 30]
video number : 68
video number + exemplar : 68
DataLoader Constructed
Test: [0/5]	Time 4.328 (4.328)	Prec@1 87.500 (87.500)
Testing Results: Prec@1 92.647
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 87.500 (87.500)
Testing Results (NME): Prec@1 83.824
Eval Task 17 for Age 22
Current Task : [20, 41]
video number : 82
video number + exemplar : 82
DataLoader Constructed
Test: [0/6]	Time 4.444 (4.444)	Prec@1 68.750 (68.750)
Testing Results: Prec@1 85.366
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 84.146
Eval Task 18 for Age 22
Current Task : [58, 42]
video number : 78
video number + exemplar : 78
DataLoader Constructed
Test: [0/5]	Time 3.530 (3.530)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 65.385
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 62.500 (62.500)
Testing Results (NME): Prec@1 61.538
Eval Task 19 for Age 22
Current Task : [60, 36]
video number : 77
video number + exemplar : 77
DataLoader Constructed
Test: [0/5]	Time 4.130 (4.130)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 67.532
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 62.500 (62.500)
Testing Results (NME): Prec@1 74.026
Eval Task 20 for Age 22
Current Task : [40, 45]
video number : 75
video number + exemplar : 75
DataLoader Constructed
Test: [0/5]	Time 4.015 (4.015)	Prec@1 81.250 (81.250)
Testing Results: Prec@1 86.667
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 93.750 (93.750)
Testing Results (NME): Prec@1 84.000
Eval Task 21 for Age 22
Current Task : [89, 0]
video number : 83
video number + exemplar : 83
DataLoader Constructed
Test: [0/6]	Time 4.413 (4.413)	Prec@1 68.750 (68.750)
Testing Results: Prec@1 80.723
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 59.036
Eval Task 22 for Age 22
Current Task : [61, 1]
video number : 80
video number + exemplar : 80
DataLoader Constructed
Test: [0/5]	Time 3.680 (3.680)	Prec@1 100.000 (100.000)
Testing Results: Prec@1 93.750
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 87.500
num_test_videos [1909, 70, 56, 73, 62, 92, 84, 84, 84, 78, 72, 68, 62, 70, 83, 68, 68, 82, 78, 77, 75, 83, 80]
Method : OURS
----AGE 23----
current_task  [92, 94]
current_head  97
Phase 2 : Train RGB Model in an Incremental Manner
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Load the Previous Model
Copy the old Model
lambda_0  : [1.0, 0.06892024376045111]
Increment the Model
SplitCosineLinear(
  input_features=512, output_features=291, sigma=tensor([3.9231]), eta=tensor([3.1036])
  (fc1): CosineLinear(input_features=512, output_features=285, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
video number : 185
video number + exemplar : 660
DataLoader Constructed : Train 20
Optimizer Constructed
video number : 185
video number + exemplar : 185
Initialize Cosine Classifier
Computing the class mean vectors...
2022-03-23 12:06:11.933950
Epoch: [0][0/20], lr: 0.00100	Time 3.857 (3.857)	Data 2.092 (2.092)	Loss 0.1425 (0.1425)	Loss CE 0.0161 (0.0161)	Loss KD (Logit) 0.9910 (0.9910)	Loss KD (GCAM) 0.0085 (0.0085)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5551 (0.5551)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
/home/ustc/anaconda3/envs/lhc/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Sigma : Parameter containing:
tensor([3.9221], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1029], device='cuda:0', requires_grad=True)
2022-03-23 12:06:35.756684
Epoch: [1][0/20], lr: 0.00100	Time 3.799 (3.799)	Data 2.630 (2.630)	Loss 0.1412 (0.1412)	Loss CE 0.0096 (0.0096)	Loss KD (Logit) 0.9952 (0.9952)	Loss KD (GCAM) 0.0106 (0.0106)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5976 (0.5976)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9097], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.0955], device='cuda:0', requires_grad=True)
2022-03-23 12:06:59.456005
Epoch: [2][0/20], lr: 0.00100	Time 3.800 (3.800)	Data 2.440 (2.440)	Loss 0.2110 (0.2110)	Loss CE 0.0723 (0.0723)	Loss KD (Logit) 1.0464 (1.0464)	Loss KD (GCAM) 0.0142 (0.0142)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6226 (0.6226)	Loss REG 0.0000 (0.0000)	Prec@1 93.750 (93.750)
Sigma : Parameter containing:
tensor([3.9060], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.0940], device='cuda:0', requires_grad=True)
2022-03-23 12:07:21.543177
Epoch: [3][0/20], lr: 0.00100	Time 3.611 (3.611)	Data 2.219 (2.219)	Loss 0.1799 (0.1799)	Loss CE 0.0385 (0.0385)	Loss KD (Logit) 1.0818 (1.0818)	Loss KD (GCAM) 0.0164 (0.0164)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6193 (0.6193)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9138], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.0992], device='cuda:0', requires_grad=True)
2022-03-23 12:07:42.899548
Epoch: [4][0/20], lr: 0.00100	Time 3.392 (3.392)	Data 2.624 (2.624)	Loss 0.2228 (0.2228)	Loss CE 0.0852 (0.0852)	Loss KD (Logit) 0.9939 (0.9939)	Loss KD (GCAM) 0.0156 (0.0156)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6441 (0.6441)	Loss REG 0.0000 (0.0000)	Prec@1 96.875 (96.875)
Sigma : Parameter containing:
tensor([3.9107], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.0980], device='cuda:0', requires_grad=True)
2022-03-23 12:08:04.126681
Epoch: [5][0/20], lr: 0.00100	Time 3.540 (3.540)	Data 2.464 (2.464)	Loss 0.1850 (0.1850)	Loss CE 0.0416 (0.0416)	Loss KD (Logit) 1.0623 (1.0623)	Loss KD (GCAM) 0.0192 (0.0192)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6445 (0.6445)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9139], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1002], device='cuda:0', requires_grad=True)
2022-03-23 12:08:25.537972
Epoch: [6][0/20], lr: 0.00100	Time 3.624 (3.624)	Data 2.584 (2.584)	Loss 0.1447 (0.1447)	Loss CE 0.0056 (0.0056)	Loss KD (Logit) 1.0149 (1.0149)	Loss KD (GCAM) 0.0179 (0.0179)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6384 (0.6384)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9165], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1020], device='cuda:0', requires_grad=True)
2022-03-23 12:08:49.288405
Epoch: [7][0/20], lr: 0.00100	Time 3.797 (3.797)	Data 2.672 (2.672)	Loss 0.1383 (0.1383)	Loss CE 0.0045 (0.0045)	Loss KD (Logit) 1.0021 (1.0021)	Loss KD (GCAM) 0.0178 (0.0178)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5945 (0.5945)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9178], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1027], device='cuda:0', requires_grad=True)
2022-03-23 12:09:12.891525
Epoch: [8][0/20], lr: 0.00100	Time 3.574 (3.574)	Data 2.050 (2.050)	Loss 0.1450 (0.1450)	Loss CE 0.0142 (0.0142)	Loss KD (Logit) 1.0352 (1.0352)	Loss KD (GCAM) 0.0185 (0.0185)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5388 (0.5388)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9227], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1056], device='cuda:0', requires_grad=True)
2022-03-23 12:09:36.069081
Epoch: [9][0/20], lr: 0.00100	Time 3.486 (3.486)	Data 2.376 (2.376)	Loss 0.1323 (0.1323)	Loss CE 0.0013 (0.0013)	Loss KD (Logit) 0.9876 (0.9876)	Loss KD (GCAM) 0.0173 (0.0173)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5769 (0.5769)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9200], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1039], device='cuda:0', requires_grad=True)
2022-03-23 12:09:57.673509
Epoch: [10][0/20], lr: 0.00100	Time 3.468 (3.468)	Data 2.190 (2.190)	Loss 0.1417 (0.1417)	Loss CE 0.0031 (0.0031)	Loss KD (Logit) 1.0123 (1.0123)	Loss KD (GCAM) 0.0176 (0.0176)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6359 (0.6359)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9250], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1068], device='cuda:0', requires_grad=True)
2022-03-23 12:10:18.947465
Epoch: [11][0/20], lr: 0.00100	Time 3.390 (3.390)	Data 2.194 (2.194)	Loss 0.1320 (0.1320)	Loss CE 0.0007 (0.0007)	Loss KD (Logit) 1.0025 (1.0025)	Loss KD (GCAM) 0.0186 (0.0186)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5663 (0.5663)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9288], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1088], device='cuda:0', requires_grad=True)
2022-03-23 12:10:39.811914
Epoch: [12][0/20], lr: 0.00100	Time 3.267 (3.267)	Data 2.200 (2.200)	Loss 0.1442 (0.1442)	Loss CE 0.0071 (0.0071)	Loss KD (Logit) 1.0313 (1.0313)	Loss KD (GCAM) 0.0177 (0.0177)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6076 (0.6076)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9332], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1113], device='cuda:0', requires_grad=True)
2022-03-23 12:11:02.210686
Epoch: [13][0/20], lr: 0.00100	Time 4.059 (4.059)	Data 2.916 (2.916)	Loss 0.1457 (0.1457)	Loss CE 0.0030 (0.0030)	Loss KD (Logit) 1.0203 (1.0203)	Loss KD (GCAM) 0.0175 (0.0175)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6710 (0.6710)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9368], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1132], device='cuda:0', requires_grad=True)
2022-03-23 12:11:25.788568
Epoch: [14][0/20], lr: 0.00100	Time 3.643 (3.643)	Data 2.062 (2.062)	Loss 0.1444 (0.1444)	Loss CE 0.0102 (0.0102)	Loss KD (Logit) 1.0001 (1.0001)	Loss KD (GCAM) 0.0171 (0.0171)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6006 (0.6006)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9401], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1152], device='cuda:0', requires_grad=True)
2022-03-23 12:11:49.331344
Epoch: [15][0/20], lr: 0.00100	Time 3.653 (3.653)	Data 2.050 (2.050)	Loss 0.1508 (0.1508)	Loss CE 0.0121 (0.0121)	Loss KD (Logit) 1.0279 (1.0279)	Loss KD (GCAM) 0.0167 (0.0167)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6289 (0.6289)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9437], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1173], device='cuda:0', requires_grad=True)
2022-03-23 12:12:12.861044
Epoch: [16][0/20], lr: 0.00100	Time 3.568 (3.568)	Data 2.223 (2.223)	Loss 0.1397 (0.1397)	Loss CE 0.0027 (0.0027)	Loss KD (Logit) 1.0145 (1.0145)	Loss KD (GCAM) 0.0168 (0.0168)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6197 (0.6197)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9470], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1192], device='cuda:0', requires_grad=True)
2022-03-23 12:12:34.934766
Epoch: [17][0/20], lr: 0.00100	Time 3.590 (3.590)	Data 2.742 (2.742)	Loss 0.1346 (0.1346)	Loss CE 0.0005 (0.0005)	Loss KD (Logit) 0.9832 (0.9832)	Loss KD (GCAM) 0.0168 (0.0168)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6132 (0.6132)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9492], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1202], device='cuda:0', requires_grad=True)
2022-03-23 12:12:56.522494
Epoch: [18][0/20], lr: 0.00100	Time 3.540 (3.540)	Data 2.238 (2.238)	Loss 0.1358 (0.1358)	Loss CE 0.0035 (0.0035)	Loss KD (Logit) 1.0175 (1.0175)	Loss KD (GCAM) 0.0170 (0.0170)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5707 (0.5707)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9509], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1214], device='cuda:0', requires_grad=True)
2022-03-23 12:13:16.170133
Epoch: [19][0/20], lr: 0.00100	Time 3.254 (3.254)	Data 2.111 (2.111)	Loss 0.1343 (0.1343)	Loss CE 0.0031 (0.0031)	Loss KD (Logit) 0.9762 (0.9762)	Loss KD (GCAM) 0.0172 (0.0172)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5884 (0.5884)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9543], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1235], device='cuda:0', requires_grad=True)
2022-03-23 12:13:37.936890
Epoch: [20][0/20], lr: 0.00010	Time 3.722 (3.722)	Data 2.645 (2.645)	Loss 0.1306 (0.1306)	Loss CE 0.0006 (0.0006)	Loss KD (Logit) 0.9689 (0.9689)	Loss KD (GCAM) 0.0154 (0.0154)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5857 (0.5857)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9545], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1237], device='cuda:0', requires_grad=True)
2022-03-23 12:14:01.154299
Epoch: [21][0/20], lr: 0.00010	Time 3.622 (3.622)	Data 2.172 (2.172)	Loss 0.1325 (0.1325)	Loss CE 0.0012 (0.0012)	Loss KD (Logit) 0.9796 (0.9796)	Loss KD (GCAM) 0.0153 (0.0153)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5926 (0.5926)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9547], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1238], device='cuda:0', requires_grad=True)
2022-03-23 12:14:24.466512
Epoch: [22][0/20], lr: 0.00010	Time 3.717 (3.717)	Data 2.418 (2.418)	Loss 0.1312 (0.1312)	Loss CE 0.0003 (0.0003)	Loss KD (Logit) 0.9838 (0.9838)	Loss KD (GCAM) 0.0164 (0.0164)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5809 (0.5809)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9548], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1238], device='cuda:0', requires_grad=True)
2022-03-23 12:14:47.775422
Epoch: [23][0/20], lr: 0.00010	Time 3.541 (3.541)	Data 2.345 (2.345)	Loss 0.1400 (0.1400)	Loss CE 0.0006 (0.0006)	Loss KD (Logit) 0.9903 (0.9903)	Loss KD (GCAM) 0.0151 (0.0151)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6665 (0.6665)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9549], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1239], device='cuda:0', requires_grad=True)
2022-03-23 12:15:08.180040
Epoch: [24][0/20], lr: 0.00010	Time 3.620 (3.620)	Data 2.615 (2.615)	Loss 0.1365 (0.1365)	Loss CE 0.0005 (0.0005)	Loss KD (Logit) 1.0117 (1.0117)	Loss KD (GCAM) 0.0154 (0.0154)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6166 (0.6166)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9551], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1240], device='cuda:0', requires_grad=True)
2022-03-23 12:15:28.867651
Epoch: [25][0/20], lr: 0.00010	Time 4.012 (4.012)	Data 3.153 (3.153)	Loss 0.1302 (0.1302)	Loss CE 0.0016 (0.0016)	Loss KD (Logit) 0.9692 (0.9692)	Loss KD (GCAM) 0.0168 (0.0168)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5675 (0.5675)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9552], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1240], device='cuda:0', requires_grad=True)
2022-03-23 12:15:48.535713
Epoch: [26][0/20], lr: 0.00010	Time 3.059 (3.059)	Data 2.119 (2.119)	Loss 0.1397 (0.1397)	Loss CE 0.0054 (0.0054)	Loss KD (Logit) 0.9900 (0.9900)	Loss KD (GCAM) 0.0153 (0.0153)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6153 (0.6153)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9553], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1241], device='cuda:0', requires_grad=True)
2022-03-23 12:16:08.809751
Epoch: [27][0/20], lr: 0.00010	Time 3.857 (3.857)	Data 2.773 (2.773)	Loss 0.1292 (0.1292)	Loss CE 0.0012 (0.0012)	Loss KD (Logit) 0.9648 (0.9648)	Loss KD (GCAM) 0.0153 (0.0153)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5682 (0.5682)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9554], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1242], device='cuda:0', requires_grad=True)
2022-03-23 12:16:31.976588
Epoch: [28][0/20], lr: 0.00010	Time 3.483 (3.483)	Data 1.858 (1.858)	Loss 0.1407 (0.1407)	Loss CE 0.0046 (0.0046)	Loss KD (Logit) 1.0111 (1.0111)	Loss KD (GCAM) 0.0148 (0.0148)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6191 (0.6191)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9556], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1242], device='cuda:0', requires_grad=True)
2022-03-23 12:16:55.239125
Epoch: [29][0/20], lr: 0.00010	Time 3.714 (3.714)	Data 2.483 (2.483)	Loss 0.1412 (0.1412)	Loss CE 0.0022 (0.0022)	Loss KD (Logit) 0.9779 (0.9779)	Loss KD (GCAM) 0.0159 (0.0159)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6686 (0.6686)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9557], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:17:18.457782
Epoch: [30][0/20], lr: 0.00001	Time 3.411 (3.411)	Data 1.978 (1.978)	Loss 0.1448 (0.1448)	Loss CE 0.0114 (0.0114)	Loss KD (Logit) 0.9542 (0.9542)	Loss KD (GCAM) 0.0157 (0.0157)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6294 (0.6294)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9557], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:17:38.876413
Epoch: [31][0/20], lr: 0.00001	Time 3.528 (3.528)	Data 2.363 (2.363)	Loss 0.1368 (0.1368)	Loss CE 0.0025 (0.0025)	Loss KD (Logit) 0.9969 (0.9969)	Loss KD (GCAM) 0.0147 (0.0147)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6120 (0.6120)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9558], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:17:59.186810
Epoch: [32][0/20], lr: 0.00001	Time 3.460 (3.460)	Data 2.349 (2.349)	Loss 0.1345 (0.1345)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.9742 (0.9742)	Loss KD (GCAM) 0.0152 (0.0152)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6264 (0.6264)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9557], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:18:19.078626
Epoch: [33][0/20], lr: 0.00001	Time 3.223 (3.223)	Data 2.372 (2.372)	Loss 0.1386 (0.1386)	Loss CE 0.0036 (0.0036)	Loss KD (Logit) 0.9973 (0.9973)	Loss KD (GCAM) 0.0146 (0.0146)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6188 (0.6188)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9558], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:18:36.939584
Epoch: [34][0/20], lr: 0.00001	Time 3.573 (3.573)	Data 2.251 (2.251)	Loss 0.1364 (0.1364)	Loss CE 0.0005 (0.0005)	Loss KD (Logit) 0.9771 (0.9771)	Loss KD (GCAM) 0.0146 (0.0146)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6417 (0.6417)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9558], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:19:00.047529
Epoch: [35][0/20], lr: 0.00001	Time 3.826 (3.826)	Data 2.798 (2.798)	Loss 0.1378 (0.1378)	Loss CE 0.0011 (0.0011)	Loss KD (Logit) 0.9592 (0.9592)	Loss KD (GCAM) 0.0151 (0.0151)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6611 (0.6611)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9557], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:19:23.401410
Epoch: [36][0/20], lr: 0.00001	Time 3.726 (3.726)	Data 2.368 (2.368)	Loss 0.1292 (0.1292)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.9616 (0.9616)	Loss KD (GCAM) 0.0153 (0.0153)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5818 (0.5818)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9558], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:19:46.554722
Epoch: [37][0/20], lr: 0.00001	Time 3.509 (3.509)	Data 2.100 (2.100)	Loss 0.1390 (0.1390)	Loss CE 0.0095 (0.0095)	Loss KD (Logit) 0.9637 (0.9637)	Loss KD (GCAM) 0.0149 (0.0149)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5857 (0.5857)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9558], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:20:08.158575
Epoch: [38][0/20], lr: 0.00001	Time 3.499 (3.499)	Data 2.531 (2.531)	Loss 0.1428 (0.1428)	Loss CE 0.0099 (0.0099)	Loss KD (Logit) 0.9674 (0.9674)	Loss KD (GCAM) 0.0156 (0.0156)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6157 (0.6157)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9558], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:20:28.801617
Epoch: [39][0/20], lr: 0.00001	Time 3.421 (3.421)	Data 1.924 (1.924)	Loss 0.1420 (0.1420)	Loss CE 0.0067 (0.0067)	Loss KD (Logit) 0.9757 (0.9757)	Loss KD (GCAM) 0.0152 (0.0152)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6356 (0.6356)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9558], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:20:49.211855
Epoch: [40][0/20], lr: 0.00001	Time 3.587 (3.587)	Data 2.763 (2.763)	Loss 0.1399 (0.1399)	Loss CE 0.0003 (0.0003)	Loss KD (Logit) 1.0213 (1.0213)	Loss KD (GCAM) 0.0154 (0.0154)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6451 (0.6451)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9558], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:21:07.579107
Epoch: [41][0/20], lr: 0.00001	Time 3.348 (3.348)	Data 1.959 (1.959)	Loss 0.1342 (0.1342)	Loss CE 0.0016 (0.0016)	Loss KD (Logit) 0.9608 (0.9608)	Loss KD (GCAM) 0.0151 (0.0151)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6193 (0.6193)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9558], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:21:30.412260
Epoch: [42][0/20], lr: 0.00001	Time 3.404 (3.404)	Data 1.924 (1.924)	Loss 0.1364 (0.1364)	Loss CE 0.0009 (0.0009)	Loss KD (Logit) 1.0113 (1.0113)	Loss KD (GCAM) 0.0156 (0.0156)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6111 (0.6111)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9558], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1244], device='cuda:0', requires_grad=True)
2022-03-23 12:21:53.761865
Epoch: [43][0/20], lr: 0.00001	Time 3.608 (3.608)	Data 2.371 (2.371)	Loss 0.1309 (0.1309)	Loss CE 0.0038 (0.0038)	Loss KD (Logit) 0.9673 (0.9673)	Loss KD (GCAM) 0.0178 (0.0178)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5510 (0.5510)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9559], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1244], device='cuda:0', requires_grad=True)
2022-03-23 12:22:17.577616
Epoch: [44][0/20], lr: 0.00001	Time 4.083 (4.083)	Data 2.889 (2.889)	Loss 0.1405 (0.1405)	Loss CE 0.0041 (0.0041)	Loss KD (Logit) 1.0117 (1.0117)	Loss KD (GCAM) 0.0155 (0.0155)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6209 (0.6209)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9559], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1244], device='cuda:0', requires_grad=True)
2022-03-23 12:22:40.346835
Epoch: [45][0/20], lr: 0.00001	Time 3.346 (3.346)	Data 2.232 (2.232)	Loss 0.1422 (0.1422)	Loss CE 0.0082 (0.0082)	Loss KD (Logit) 0.9957 (0.9957)	Loss KD (GCAM) 0.0167 (0.0167)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6033 (0.6033)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9558], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:23:02.333975
Epoch: [46][0/20], lr: 0.00001	Time 3.421 (3.421)	Data 2.198 (2.198)	Loss 0.1251 (0.1251)	Loss CE 0.0004 (0.0004)	Loss KD (Logit) 0.9528 (0.9528)	Loss KD (GCAM) 0.0163 (0.0163)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5411 (0.5411)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9559], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1244], device='cuda:0', requires_grad=True)
2022-03-23 12:23:24.428812
Epoch: [47][0/20], lr: 0.00001	Time 3.819 (3.819)	Data 2.449 (2.449)	Loss 0.1336 (0.1336)	Loss CE 0.0074 (0.0074)	Loss KD (Logit) 0.9509 (0.9509)	Loss KD (GCAM) 0.0157 (0.0157)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5593 (0.5593)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9559], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1244], device='cuda:0', requires_grad=True)
2022-03-23 12:23:44.639496
Epoch: [48][0/20], lr: 0.00001	Time 3.690 (3.690)	Data 2.493 (2.493)	Loss 0.1520 (0.1520)	Loss CE 0.0217 (0.0217)	Loss KD (Logit) 0.9747 (0.9747)	Loss KD (GCAM) 0.0151 (0.0151)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5859 (0.5859)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9559], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1244], device='cuda:0', requires_grad=True)
2022-03-23 12:24:07.875369
Epoch: [49][0/20], lr: 0.00001	Time 3.499 (3.499)	Data 2.192 (2.192)	Loss 0.1374 (0.1374)	Loss CE 0.0046 (0.0046)	Loss KD (Logit) 1.0022 (1.0022)	Loss KD (GCAM) 0.0163 (0.0163)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5886 (0.5886)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9559], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1244], device='cuda:0', requires_grad=True)
Update Importance Mask...
Parameter containing:
tensor([0.0946], device='cuda:0', requires_grad=True) Parameter containing:
tensor([0.1891], device='cuda:0', requires_grad=True) Parameter containing:
tensor([0.2839], device='cuda:0', requires_grad=True) Parameter containing:
tensor([0.3782], device='cuda:0', requires_grad=True) Phase 3 : Manage Exemplar Sets
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Construct Exemplar Set
Load the Model
SplitCosineLinear(
  input_features=512, output_features=291, sigma=tensor([3.9559]), eta=tensor([3.1244])
  (fc1): CosineLinear(input_features=512, output_features=285, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
Exemplar per class : 5
video number : 185
video number + exemplar : 185
Phase 4 : Class-balanced Fine-Tuning
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Load the Model
SplitCosineLinear(
  input_features=512, output_features=291, sigma=tensor([3.9559]), eta=tensor([3.1244])
  (fc1): CosineLinear(input_features=512, output_features=285, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
exemplar : 485
DataLoader CBF Constructed : Train 15
Optimizer Constructed
2022-03-23 12:24:52.347603
Epoch: [0][0/15], lr: 0.00050	Time 3.212 (3.212)	Data 2.292 (2.292)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9560], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1243], device='cuda:0', requires_grad=True)
2022-03-23 12:25:05.883273
Epoch: [1][0/15], lr: 0.00050	Time 3.630 (3.630)	Data 2.601 (2.601)	Loss 0.0045 (0.0045)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9562], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1242], device='cuda:0', requires_grad=True)
2022-03-23 12:25:17.558137
Epoch: [2][0/15], lr: 0.00050	Time 3.150 (3.150)	Data 2.081 (2.081)	Loss 0.0029 (0.0029)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9566], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1242], device='cuda:0', requires_grad=True)
2022-03-23 12:25:29.899482
Epoch: [3][0/15], lr: 0.00050	Time 3.326 (3.326)	Data 2.682 (2.682)	Loss 0.0012 (0.0012)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9566], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1238], device='cuda:0', requires_grad=True)
2022-03-23 12:25:41.729866
Epoch: [4][0/15], lr: 0.00050	Time 3.268 (3.268)	Data 2.434 (2.434)	Loss 0.0007 (0.0007)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9545], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1222], device='cuda:0', requires_grad=True)
2022-03-23 12:25:53.342300
Epoch: [5][0/15], lr: 0.00050	Time 3.297 (3.297)	Data 2.559 (2.559)	Loss 0.0001 (0.0001)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9534], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1213], device='cuda:0', requires_grad=True)
2022-03-23 12:26:05.070314
Epoch: [6][0/15], lr: 0.00050	Time 3.148 (3.148)	Data 2.498 (2.498)	Loss 0.0016 (0.0016)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9532], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1210], device='cuda:0', requires_grad=True)
2022-03-23 12:26:15.804979
Epoch: [7][0/15], lr: 0.00050	Time 2.918 (2.918)	Data 1.915 (1.915)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9530], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1207], device='cuda:0', requires_grad=True)
2022-03-23 12:26:27.989267
Epoch: [8][0/15], lr: 0.00050	Time 3.313 (3.313)	Data 2.162 (2.162)	Loss 0.0008 (0.0008)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9532], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1206], device='cuda:0', requires_grad=True)
2022-03-23 12:26:41.405315
Epoch: [9][0/15], lr: 0.00050	Time 3.714 (3.714)	Data 3.027 (3.027)	Loss 0.0025 (0.0025)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9534], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1205], device='cuda:0', requires_grad=True)
2022-03-23 12:26:54.762330
Epoch: [10][0/15], lr: 0.00050	Time 3.504 (3.504)	Data 2.697 (2.697)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9532], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1202], device='cuda:0', requires_grad=True)
2022-03-23 12:27:07.237734
Epoch: [11][0/15], lr: 0.00050	Time 2.980 (2.980)	Data 2.048 (2.048)	Loss 0.0009 (0.0009)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9529], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1198], device='cuda:0', requires_grad=True)
2022-03-23 12:27:20.274105
Epoch: [12][0/15], lr: 0.00050	Time 3.284 (3.284)	Data 2.145 (2.145)	Loss 0.0003 (0.0003)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9524], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1194], device='cuda:0', requires_grad=True)
2022-03-23 12:27:33.282846
Epoch: [13][0/15], lr: 0.00050	Time 3.156 (3.156)	Data 2.197 (2.197)	Loss 0.0003 (0.0003)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9521], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1190], device='cuda:0', requires_grad=True)
2022-03-23 12:27:46.381970
Epoch: [14][0/15], lr: 0.00050	Time 3.350 (3.350)	Data 2.801 (2.801)	Loss 0.0061 (0.0061)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9523], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1190], device='cuda:0', requires_grad=True)
2022-03-23 12:27:58.206377
Epoch: [15][0/15], lr: 0.00050	Time 3.363 (3.363)	Data 2.637 (2.637)	Loss 0.0006 (0.0006)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9524], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1189], device='cuda:0', requires_grad=True)
2022-03-23 12:28:10.279783
Epoch: [16][0/15], lr: 0.00050	Time 2.993 (2.993)	Data 1.935 (1.935)	Loss 0.0009 (0.0009)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9525], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1187], device='cuda:0', requires_grad=True)
2022-03-23 12:28:22.068473
Epoch: [17][0/15], lr: 0.00050	Time 3.113 (3.113)	Data 2.411 (2.411)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9522], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1183], device='cuda:0', requires_grad=True)
2022-03-23 12:28:33.588044
Epoch: [18][0/15], lr: 0.00050	Time 2.927 (2.927)	Data 2.179 (2.179)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9517], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1179], device='cuda:0', requires_grad=True)
2022-03-23 12:28:45.254910
Epoch: [19][0/15], lr: 0.00050	Time 2.960 (2.960)	Data 2.310 (2.310)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9516], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1176], device='cuda:0', requires_grad=True)
Phase 5 : Eval RGB Model for the Tasks Trained so far
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Load the Trained Model from checkpoint/ucf101/51/2/005/task_023.pth.tar
exemplar : 485
Computing the class mean vectors...
Eval Task 0 for Age 23
Current Task : [37, 97, 56, 55, 33, 84, 3, 4, 72, 59, 66, 48, 65, 91, 99, 39, 34, 22, 67, 74, 19, 35, 9, 86, 88, 63, 85, 38, 54, 25, 57, 62, 83, 76, 6, 13, 2, 53, 8, 24, 44, 12, 100, 29, 5, 17, 15, 73, 47, 27, 46]
video number : 1909
video number + exemplar : 1909
DataLoader Constructed
Test: [0/120]	Time 4.984 (4.984)	Prec@1 75.000 (75.000)
Test: [100/120]	Time 0.554 (0.594)	Prec@1 62.500 (57.054)
Testing Results: Prec@1 58.041
Classify using the NME Classifier...
Test (NME): [0/120]	Time 0.000 (0.000)	Prec@1 62.500 (62.500)
Test (NME): [100/120]	Time 0.000 (0.000)	Prec@1 62.500 (66.584)
Testing Results (NME): Prec@1 66.946
Eval Task 1 for Age 23
Current Task : [98, 96]
video number : 70
video number + exemplar : 70
DataLoader Constructed
Test: [0/5]	Time 4.422 (4.422)	Prec@1 56.250 (56.250)
Testing Results: Prec@1 55.714
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 64.286
Eval Task 2 for Age 23
Current Task : [18, 90]
video number : 56
video number + exemplar : 56
DataLoader Constructed
Test: [0/4]	Time 3.994 (3.994)	Prec@1 43.750 (43.750)
Testing Results: Prec@1 60.714
Classify using the NME Classifier...
Test (NME): [0/4]	Time 0.000 (0.000)	Prec@1 62.500 (62.500)
Testing Results (NME): Prec@1 62.500
Eval Task 3 for Age 23
Current Task : [75, 31]
video number : 73
video number + exemplar : 73
DataLoader Constructed
Test: [0/5]	Time 4.085 (4.085)	Prec@1 75.000 (75.000)
Testing Results: Prec@1 71.233
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 72.603
Eval Task 4 for Age 23
Current Task : [95, 49]
video number : 62
video number + exemplar : 62
DataLoader Constructed
Test: [0/4]	Time 3.984 (3.984)	Prec@1 68.750 (68.750)
Testing Results: Prec@1 75.806
Classify using the NME Classifier...
Test (NME): [0/4]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 87.097
Eval Task 5 for Age 23
Current Task : [43, 78]
video number : 92
video number + exemplar : 92
DataLoader Constructed
Test: [0/6]	Time 4.500 (4.500)	Prec@1 43.750 (43.750)
Testing Results: Prec@1 50.000
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 75.000 (75.000)
Testing Results (NME): Prec@1 71.739
Eval Task 6 for Age 23
Current Task : [23, 68]
video number : 84
video number + exemplar : 84
DataLoader Constructed
Test: [0/6]	Time 3.767 (3.767)	Prec@1 37.500 (37.500)
Testing Results: Prec@1 58.333
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 43.750 (43.750)
Testing Results (NME): Prec@1 51.190
Eval Task 7 for Age 23
Current Task : [16, 7]
video number : 84
video number + exemplar : 84
DataLoader Constructed
Test: [0/6]	Time 4.576 (4.576)	Prec@1 12.500 (12.500)
Testing Results: Prec@1 27.381
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 12.500 (12.500)
Testing Results (NME): Prec@1 27.381
Eval Task 8 for Age 23
Current Task : [26, 21]
video number : 84
video number + exemplar : 84
DataLoader Constructed
Test: [0/6]	Time 4.429 (4.429)	Prec@1 81.250 (81.250)
Testing Results: Prec@1 82.143
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 75.000
Eval Task 9 for Age 23
Current Task : [50, 70]
video number : 78
video number + exemplar : 78
DataLoader Constructed
Test: [0/5]	Time 4.275 (4.275)	Prec@1 75.000 (75.000)
Testing Results: Prec@1 76.923
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 87.500 (87.500)
Testing Results (NME): Prec@1 80.769
Eval Task 10 for Age 23
Current Task : [32, 52]
video number : 72
video number + exemplar : 72
DataLoader Constructed
Test: [0/5]	Time 4.228 (4.228)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 54.167
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 62.500 (62.500)
Testing Results (NME): Prec@1 66.667
Eval Task 11 for Age 23
Current Task : [11, 69]
video number : 68
video number + exemplar : 68
DataLoader Constructed
Test: [0/5]	Time 3.931 (3.931)	Prec@1 68.750 (68.750)
Testing Results: Prec@1 73.529
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 77.941
Eval Task 12 for Age 23
Current Task : [93, 14]
video number : 62
video number + exemplar : 62
DataLoader Constructed
Test: [0/4]	Time 4.128 (4.128)	Prec@1 31.250 (31.250)
Testing Results: Prec@1 37.097
Classify using the NME Classifier...
Test (NME): [0/4]	Time 0.000 (0.000)	Prec@1 31.250 (31.250)
Testing Results (NME): Prec@1 48.387
Eval Task 13 for Age 23
Current Task : [79, 10]
video number : 70
video number + exemplar : 70
DataLoader Constructed
Test: [0/5]	Time 3.948 (3.948)	Prec@1 75.000 (75.000)
Testing Results: Prec@1 81.429
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 87.500 (87.500)
Testing Results (NME): Prec@1 82.857
Eval Task 14 for Age 23
Current Task : [80, 77]
video number : 83
video number + exemplar : 83
DataLoader Constructed
Test: [0/6]	Time 4.006 (4.006)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 75.904
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 43.750 (43.750)
Testing Results (NME): Prec@1 66.265
Eval Task 15 for Age 23
Current Task : [81, 28]
video number : 68
video number + exemplar : 68
DataLoader Constructed
Test: [0/5]	Time 4.249 (4.249)	Prec@1 68.750 (68.750)
Testing Results: Prec@1 66.176
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 73.529
Eval Task 16 for Age 23
Current Task : [82, 30]
video number : 68
video number + exemplar : 68
DataLoader Constructed
Test: [0/5]	Time 4.152 (4.152)	Prec@1 93.750 (93.750)
Testing Results: Prec@1 94.118
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 87.500 (87.500)
Testing Results (NME): Prec@1 88.235
Eval Task 17 for Age 23
Current Task : [20, 41]
video number : 82
video number + exemplar : 82
DataLoader Constructed
Test: [0/6]	Time 4.704 (4.704)	Prec@1 68.750 (68.750)
Testing Results: Prec@1 81.707
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 82.927
Eval Task 18 for Age 23
Current Task : [58, 42]
video number : 78
video number + exemplar : 78
DataLoader Constructed
Test: [0/5]	Time 4.378 (4.378)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 62.821
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 66.667
Eval Task 19 for Age 23
Current Task : [60, 36]
video number : 77
video number + exemplar : 77
DataLoader Constructed
Test: [0/5]	Time 4.718 (4.718)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 79.221
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 72.727
Eval Task 20 for Age 23
Current Task : [40, 45]
video number : 75
video number + exemplar : 75
DataLoader Constructed
Test: [0/5]	Time 4.387 (4.387)	Prec@1 93.750 (93.750)
Testing Results: Prec@1 93.333
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 93.750 (93.750)
Testing Results (NME): Prec@1 88.000
Eval Task 21 for Age 23
Current Task : [89, 0]
video number : 83
video number + exemplar : 83
DataLoader Constructed
Test: [0/6]	Time 4.661 (4.661)	Prec@1 56.250 (56.250)
Testing Results: Prec@1 67.470
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 60.241
Eval Task 22 for Age 23
Current Task : [61, 1]
video number : 80
video number + exemplar : 80
DataLoader Constructed
Test: [0/5]	Time 4.723 (4.723)	Prec@1 100.000 (100.000)
Testing Results: Prec@1 91.250
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 86.250
Eval Task 23 for Age 23
Current Task : [92, 94]
video number : 81
video number + exemplar : 81
DataLoader Constructed
Test: [0/6]	Time 5.578 (5.578)	Prec@1 100.000 (100.000)
Testing Results: Prec@1 100.000
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 100.000 (100.000)
Testing Results (NME): Prec@1 86.420
num_test_videos [1909, 70, 56, 73, 62, 92, 84, 84, 84, 78, 72, 68, 62, 70, 83, 68, 68, 82, 78, 77, 75, 83, 80, 81]
Method : OURS
----AGE 24----
current_task  [64, 71]
current_head  99
Phase 2 : Train RGB Model in an Incremental Manner
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Load the Previous Model
Copy the old Model
lambda_0  : [1.0, 0.0696419413859206]
Increment the Model
SplitCosineLinear(
  input_features=512, output_features=297, sigma=tensor([3.9516]), eta=tensor([3.1176])
  (fc1): CosineLinear(input_features=512, output_features=291, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
video number : 185
video number + exemplar : 670
DataLoader Constructed : Train 20
Optimizer Constructed
video number : 185
video number + exemplar : 185
Initialize Cosine Classifier
Computing the class mean vectors...
/home/ustc/anaconda3/envs/lhc/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2022-03-23 12:36:15.960145
Epoch: [0][0/20], lr: 0.00100	Time 4.084 (4.084)	Data 2.576 (2.576)	Loss 0.0688 (0.0688)	Loss CE 0.0083 (0.0083)	Loss KD (Logit) 0.0013 (0.0013)	Loss KD (GCAM) 0.0001 (0.0001)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6036 (0.6036)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9436], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1134], device='cuda:0', requires_grad=True)
2022-03-23 12:36:36.558310
Epoch: [1][0/20], lr: 0.00100	Time 4.468 (4.468)	Data 3.136 (3.136)	Loss 0.0637 (0.0637)	Loss CE 0.0021 (0.0021)	Loss KD (Logit) 0.0013 (0.0013)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6151 (0.6151)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9458], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1153], device='cuda:0', requires_grad=True)
2022-03-23 12:37:00.662981
Epoch: [2][0/20], lr: 0.00100	Time 4.434 (4.434)	Data 3.347 (3.347)	Loss 0.1122 (0.1122)	Loss CE 0.0513 (0.0513)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6076 (0.6076)	Loss REG 0.0000 (0.0000)	Prec@1 96.875 (96.875)
Sigma : Parameter containing:
tensor([3.9413], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1129], device='cuda:0', requires_grad=True)
2022-03-23 12:37:24.613799
Epoch: [3][0/20], lr: 0.00100	Time 4.284 (4.284)	Data 2.657 (2.657)	Loss 0.0696 (0.0696)	Loss CE 0.0082 (0.0082)	Loss KD (Logit) 0.0013 (0.0013)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6124 (0.6124)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9460], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1160], device='cuda:0', requires_grad=True)
2022-03-23 12:37:47.945891
Epoch: [4][0/20], lr: 0.00100	Time 3.812 (3.812)	Data 2.804 (2.804)	Loss 0.0580 (0.0580)	Loss CE 0.0015 (0.0015)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5630 (0.5630)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9466], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1166], device='cuda:0', requires_grad=True)
2022-03-23 12:38:09.434750
Epoch: [5][0/20], lr: 0.00100	Time 4.110 (4.110)	Data 3.127 (3.127)	Loss 0.0630 (0.0630)	Loss CE 0.0036 (0.0036)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5925 (0.5925)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9474], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1171], device='cuda:0', requires_grad=True)
2022-03-23 12:38:29.305759
Epoch: [6][0/20], lr: 0.00100	Time 3.240 (3.240)	Data 2.387 (2.387)	Loss 0.0641 (0.0641)	Loss CE 0.0012 (0.0012)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6275 (0.6275)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9498], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1189], device='cuda:0', requires_grad=True)
2022-03-23 12:38:50.028910
Epoch: [7][0/20], lr: 0.00100	Time 3.862 (3.862)	Data 2.637 (2.637)	Loss 0.0823 (0.0823)	Loss CE 0.0190 (0.0190)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6319 (0.6319)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9544], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1218], device='cuda:0', requires_grad=True)
2022-03-23 12:39:09.170689
Epoch: [8][0/20], lr: 0.00100	Time 4.108 (4.108)	Data 2.874 (2.874)	Loss 0.0966 (0.0966)	Loss CE 0.0355 (0.0355)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6097 (0.6097)	Loss REG 0.0000 (0.0000)	Prec@1 96.875 (96.875)
Sigma : Parameter containing:
tensor([3.9590], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1246], device='cuda:0', requires_grad=True)
2022-03-23 12:39:33.014705
Epoch: [9][0/20], lr: 0.00100	Time 4.108 (4.108)	Data 3.005 (3.005)	Loss 0.0614 (0.0614)	Loss CE 0.0003 (0.0003)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6095 (0.6095)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9617], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1259], device='cuda:0', requires_grad=True)
2022-03-23 12:39:56.567975
Epoch: [10][0/20], lr: 0.00100	Time 4.045 (4.045)	Data 3.049 (3.049)	Loss 0.1190 (0.1190)	Loss CE 0.0597 (0.0597)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5916 (0.5916)	Loss REG 0.0000 (0.0000)	Prec@1 96.875 (96.875)
Sigma : Parameter containing:
tensor([3.9645], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1275], device='cuda:0', requires_grad=True)
2022-03-23 12:40:20.429043
Epoch: [11][0/20], lr: 0.00100	Time 4.093 (4.093)	Data 2.984 (2.984)	Loss 0.0635 (0.0635)	Loss CE 0.0004 (0.0004)	Loss KD (Logit) 0.0013 (0.0013)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6299 (0.6299)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9674], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1295], device='cuda:0', requires_grad=True)
2022-03-23 12:40:41.378503
Epoch: [12][0/20], lr: 0.00100	Time 3.934 (3.934)	Data 2.885 (2.885)	Loss 0.0670 (0.0670)	Loss CE 0.0034 (0.0034)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6342 (0.6342)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9695], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1306], device='cuda:0', requires_grad=True)
2022-03-23 12:41:02.053786
Epoch: [13][0/20], lr: 0.00100	Time 4.014 (4.014)	Data 3.165 (3.165)	Loss 0.0630 (0.0630)	Loss CE 0.0008 (0.0008)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6210 (0.6210)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9688], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1298], device='cuda:0', requires_grad=True)
2022-03-23 12:41:22.100260
Epoch: [14][0/20], lr: 0.00100	Time 3.283 (3.283)	Data 2.448 (2.448)	Loss 0.0661 (0.0661)	Loss CE 0.0051 (0.0051)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6084 (0.6084)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9696], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1300], device='cuda:0', requires_grad=True)
2022-03-23 12:41:41.224793
Epoch: [15][0/20], lr: 0.00100	Time 4.519 (4.519)	Data 3.572 (3.572)	Loss 0.0647 (0.0647)	Loss CE 0.0045 (0.0045)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6009 (0.6009)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9651], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1271], device='cuda:0', requires_grad=True)
2022-03-23 12:42:04.261710
Epoch: [16][0/20], lr: 0.00100	Time 3.613 (3.613)	Data 2.206 (2.206)	Loss 0.0585 (0.0585)	Loss CE 0.0025 (0.0025)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5576 (0.5576)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9615], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1247], device='cuda:0', requires_grad=True)
2022-03-23 12:42:28.154846
Epoch: [17][0/20], lr: 0.00100	Time 4.133 (4.133)	Data 2.962 (2.962)	Loss 0.0659 (0.0659)	Loss CE 0.0018 (0.0018)	Loss KD (Logit) 0.0013 (0.0013)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6390 (0.6390)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9642], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1261], device='cuda:0', requires_grad=True)
2022-03-23 12:42:51.729540
Epoch: [18][0/20], lr: 0.00100	Time 3.718 (3.718)	Data 2.398 (2.398)	Loss 0.0626 (0.0626)	Loss CE 0.0021 (0.0021)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6033 (0.6033)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9646], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1263], device='cuda:0', requires_grad=True)
2022-03-23 12:43:13.996517
Epoch: [19][0/20], lr: 0.00100	Time 3.717 (3.717)	Data 2.416 (2.416)	Loss 0.0575 (0.0575)	Loss CE 0.0004 (0.0004)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5688 (0.5688)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9666], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1275], device='cuda:0', requires_grad=True)
2022-03-23 12:43:35.677413
Epoch: [20][0/20], lr: 0.00010	Time 3.794 (3.794)	Data 2.831 (2.831)	Loss 0.0687 (0.0687)	Loss CE 0.0074 (0.0074)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6119 (0.6119)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9667], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1275], device='cuda:0', requires_grad=True)
2022-03-23 12:43:57.171847
Epoch: [21][0/20], lr: 0.00010	Time 3.686 (3.686)	Data 2.743 (2.743)	Loss 0.0598 (0.0598)	Loss CE 0.0004 (0.0004)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5925 (0.5925)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9669], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1276], device='cuda:0', requires_grad=True)
2022-03-23 12:44:19.037775
Epoch: [22][0/20], lr: 0.00010	Time 4.147 (4.147)	Data 3.017 (3.017)	Loss 0.0608 (0.0608)	Loss CE 0.0004 (0.0004)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6032 (0.6032)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9671], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:44:42.639340
Epoch: [23][0/20], lr: 0.00010	Time 3.869 (3.869)	Data 2.553 (2.553)	Loss 0.0621 (0.0621)	Loss CE 0.0015 (0.0015)	Loss KD (Logit) 0.0013 (0.0013)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6043 (0.6043)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9673], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1278], device='cuda:0', requires_grad=True)
2022-03-23 12:45:06.342138
Epoch: [24][0/20], lr: 0.00010	Time 3.849 (3.849)	Data 2.590 (2.590)	Loss 0.0648 (0.0648)	Loss CE 0.0061 (0.0061)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5853 (0.5853)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9674], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1279], device='cuda:0', requires_grad=True)
2022-03-23 12:45:29.950059
Epoch: [25][0/20], lr: 0.00010	Time 3.673 (3.673)	Data 2.442 (2.442)	Loss 0.0590 (0.0590)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5859 (0.5859)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9676], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1280], device='cuda:0', requires_grad=True)
2022-03-23 12:45:51.925398
Epoch: [26][0/20], lr: 0.00010	Time 3.748 (3.748)	Data 2.803 (2.803)	Loss 0.0640 (0.0640)	Loss CE 0.0010 (0.0010)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6277 (0.6277)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9677], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1281], device='cuda:0', requires_grad=True)
2022-03-23 12:46:13.326411
Epoch: [27][0/20], lr: 0.00010	Time 3.600 (3.600)	Data 2.677 (2.677)	Loss 0.0605 (0.0605)	Loss CE 0.0010 (0.0010)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5931 (0.5931)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9675], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1279], device='cuda:0', requires_grad=True)
2022-03-23 12:46:34.438486
Epoch: [28][0/20], lr: 0.00010	Time 3.491 (3.491)	Data 2.384 (2.384)	Loss 0.0611 (0.0611)	Loss CE 0.0011 (0.0011)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5990 (0.5990)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9676], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1280], device='cuda:0', requires_grad=True)
2022-03-23 12:46:56.059828
Epoch: [29][0/20], lr: 0.00010	Time 3.947 (3.947)	Data 2.899 (2.899)	Loss 0.0621 (0.0621)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6176 (0.6176)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9673], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:47:20.345447
Epoch: [30][0/20], lr: 0.00001	Time 4.065 (4.065)	Data 2.923 (2.923)	Loss 0.0620 (0.0620)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.0013 (0.0013)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6160 (0.6160)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9673], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:47:44.266581
Epoch: [31][0/20], lr: 0.00001	Time 3.753 (3.753)	Data 2.654 (2.654)	Loss 0.0639 (0.0639)	Loss CE 0.0016 (0.0016)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6211 (0.6211)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9673], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:48:08.369351
Epoch: [32][0/20], lr: 0.00001	Time 3.755 (3.755)	Data 2.821 (2.821)	Loss 0.0588 (0.0588)	Loss CE 0.0007 (0.0007)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5794 (0.5794)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9673], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:48:30.564375
Epoch: [33][0/20], lr: 0.00001	Time 3.826 (3.826)	Data 2.821 (2.821)	Loss 0.0569 (0.0569)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5652 (0.5652)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9673], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:48:52.931250
Epoch: [34][0/20], lr: 0.00001	Time 4.084 (4.084)	Data 2.936 (2.936)	Loss 0.0637 (0.0637)	Loss CE 0.0004 (0.0004)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6313 (0.6313)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9673], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:49:13.430742
Epoch: [35][0/20], lr: 0.00001	Time 3.129 (3.129)	Data 2.054 (2.054)	Loss 0.0616 (0.0616)	Loss CE 0.0010 (0.0010)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6035 (0.6035)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9673], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:49:36.871070
Epoch: [36][0/20], lr: 0.00001	Time 3.879 (3.879)	Data 2.418 (2.418)	Loss 0.0579 (0.0579)	Loss CE 0.0004 (0.0004)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5726 (0.5726)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9673], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:50:01.104291
Epoch: [37][0/20], lr: 0.00001	Time 4.194 (4.194)	Data 2.653 (2.653)	Loss 0.0614 (0.0614)	Loss CE 0.0007 (0.0007)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6056 (0.6056)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9674], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:50:24.515757
Epoch: [38][0/20], lr: 0.00001	Time 3.606 (3.606)	Data 2.230 (2.230)	Loss 0.0591 (0.0591)	Loss CE 0.0003 (0.0003)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5861 (0.5861)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9674], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:50:47.828763
Epoch: [39][0/20], lr: 0.00001	Time 3.820 (3.820)	Data 2.499 (2.499)	Loss 0.0710 (0.0710)	Loss CE 0.0087 (0.0087)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6204 (0.6204)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9674], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:51:09.632413
Epoch: [40][0/20], lr: 0.00001	Time 3.585 (3.585)	Data 2.279 (2.279)	Loss 0.0650 (0.0650)	Loss CE 0.0038 (0.0038)	Loss KD (Logit) 0.0013 (0.0013)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6104 (0.6104)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9674], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:51:31.180414
Epoch: [41][0/20], lr: 0.00001	Time 3.621 (3.621)	Data 2.340 (2.340)	Loss 0.0547 (0.0547)	Loss CE 0.0001 (0.0001)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5438 (0.5438)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9674], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:51:51.922886
Epoch: [42][0/20], lr: 0.00001	Time 3.739 (3.739)	Data 2.305 (2.305)	Loss 0.0628 (0.0628)	Loss CE 0.0013 (0.0013)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6125 (0.6125)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9674], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:52:15.548405
Epoch: [43][0/20], lr: 0.00001	Time 3.943 (3.943)	Data 2.990 (2.990)	Loss 0.0624 (0.0624)	Loss CE 0.0025 (0.0025)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5977 (0.5977)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9674], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:52:39.131727
Epoch: [44][0/20], lr: 0.00001	Time 3.753 (3.753)	Data 2.496 (2.496)	Loss 0.0693 (0.0693)	Loss CE 0.0081 (0.0081)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6105 (0.6105)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9674], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:53:02.646131
Epoch: [45][0/20], lr: 0.00001	Time 3.747 (3.747)	Data 2.248 (2.248)	Loss 0.0608 (0.0608)	Loss CE 0.0017 (0.0017)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5884 (0.5884)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9674], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:53:25.555192
Epoch: [46][0/20], lr: 0.00001	Time 3.636 (3.636)	Data 2.848 (2.848)	Loss 0.0629 (0.0629)	Loss CE 0.0008 (0.0008)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6197 (0.6197)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9674], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 12:53:47.852794
Epoch: [47][0/20], lr: 0.00001	Time 3.876 (3.876)	Data 2.856 (2.856)	Loss 0.0659 (0.0659)	Loss CE 0.0038 (0.0038)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6187 (0.6187)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9675], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1278], device='cuda:0', requires_grad=True)
2022-03-23 12:54:09.416495
Epoch: [48][0/20], lr: 0.00001	Time 3.808 (3.808)	Data 2.674 (2.674)	Loss 0.0650 (0.0650)	Loss CE 0.0008 (0.0008)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6406 (0.6406)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9675], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1278], device='cuda:0', requires_grad=True)
2022-03-23 12:54:29.707430
Epoch: [49][0/20], lr: 0.00001	Time 3.538 (3.538)	Data 2.024 (2.024)	Loss 0.0591 (0.0591)	Loss CE 0.0010 (0.0010)	Loss KD (Logit) 0.0014 (0.0014)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5799 (0.5799)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9675], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1278], device='cuda:0', requires_grad=True)
Update Importance Mask...
Parameter containing:
tensor([0.0944], device='cuda:0', requires_grad=True) Parameter containing:
tensor([0.1887], device='cuda:0', requires_grad=True) Parameter containing:
tensor([0.2833], device='cuda:0', requires_grad=True) Parameter containing:
tensor([0.3774], device='cuda:0', requires_grad=True) Phase 3 : Manage Exemplar Sets
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Construct Exemplar Set
Load the Model
SplitCosineLinear(
  input_features=512, output_features=297, sigma=tensor([3.9675]), eta=tensor([3.1278])
  (fc1): CosineLinear(input_features=512, output_features=291, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
Exemplar per class : 5
video number : 185
video number + exemplar : 185
Phase 4 : Class-balanced Fine-Tuning
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Load the Model
SplitCosineLinear(
  input_features=512, output_features=297, sigma=tensor([3.9675]), eta=tensor([3.1278])
  (fc1): CosineLinear(input_features=512, output_features=291, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
exemplar : 495
DataLoader CBF Constructed : Train 15
Optimizer Constructed
2022-03-23 12:55:14.112013
Epoch: [0][0/15], lr: 0.00050	Time 3.160 (3.160)	Data 2.047 (2.047)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9678], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1278], device='cuda:0', requires_grad=True)
2022-03-23 12:55:27.103898
Epoch: [1][0/15], lr: 0.00050	Time 3.387 (3.387)	Data 2.691 (2.691)	Loss 0.0090 (0.0090)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9683], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1280], device='cuda:0', requires_grad=True)
2022-03-23 12:55:40.088008
Epoch: [2][0/15], lr: 0.00050	Time 3.323 (3.323)	Data 2.418 (2.418)	Loss 0.0006 (0.0006)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9691], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1283], device='cuda:0', requires_grad=True)
2022-03-23 12:55:52.069191
Epoch: [3][0/15], lr: 0.00050	Time 3.272 (3.272)	Data 2.721 (2.721)	Loss 0.0006 (0.0006)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9697], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1284], device='cuda:0', requires_grad=True)
2022-03-23 12:56:03.848281
Epoch: [4][0/15], lr: 0.00050	Time 3.375 (3.375)	Data 2.786 (2.786)	Loss 0.0003 (0.0003)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9704], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1286], device='cuda:0', requires_grad=True)
2022-03-23 12:56:15.039019
Epoch: [5][0/15], lr: 0.00050	Time 3.215 (3.215)	Data 2.384 (2.384)	Loss 0.0004 (0.0004)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9713], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1290], device='cuda:0', requires_grad=True)
2022-03-23 12:56:26.404823
Epoch: [6][0/15], lr: 0.00050	Time 3.194 (3.194)	Data 2.311 (2.311)	Loss 0.0031 (0.0031)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9721], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1293], device='cuda:0', requires_grad=True)
2022-03-23 12:56:37.691705
Epoch: [7][0/15], lr: 0.00050	Time 3.389 (3.389)	Data 2.419 (2.419)	Loss 0.0005 (0.0005)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9728], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1295], device='cuda:0', requires_grad=True)
2022-03-23 12:56:48.362720
Epoch: [8][0/15], lr: 0.00050	Time 3.178 (3.178)	Data 2.440 (2.440)	Loss 0.0004 (0.0004)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9731], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1294], device='cuda:0', requires_grad=True)
2022-03-23 12:56:58.808965
Epoch: [9][0/15], lr: 0.00050	Time 3.261 (3.261)	Data 2.268 (2.268)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9731], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1292], device='cuda:0', requires_grad=True)
2022-03-23 12:57:11.777282
Epoch: [10][0/15], lr: 0.00050	Time 3.341 (3.341)	Data 2.367 (2.367)	Loss 0.0009 (0.0009)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9729], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1289], device='cuda:0', requires_grad=True)
2022-03-23 12:57:25.038848
Epoch: [11][0/15], lr: 0.00050	Time 3.566 (3.566)	Data 2.727 (2.727)	Loss 0.0003 (0.0003)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9727], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1286], device='cuda:0', requires_grad=True)
2022-03-23 12:57:38.246888
Epoch: [12][0/15], lr: 0.00050	Time 3.535 (3.535)	Data 2.808 (2.808)	Loss 0.0004 (0.0004)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9731], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1286], device='cuda:0', requires_grad=True)
2022-03-23 12:57:51.331810
Epoch: [13][0/15], lr: 0.00050	Time 3.482 (3.482)	Data 2.380 (2.380)	Loss 0.0001 (0.0001)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9729], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1283], device='cuda:0', requires_grad=True)
2022-03-23 12:58:04.189732
Epoch: [14][0/15], lr: 0.00050	Time 3.365 (3.365)	Data 2.301 (2.301)	Loss 0.0007 (0.0007)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9729], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1281], device='cuda:0', requires_grad=True)
2022-03-23 12:58:17.557743
Epoch: [15][0/15], lr: 0.00050	Time 3.626 (3.626)	Data 2.861 (2.861)	Loss 0.0007 (0.0007)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9732], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1281], device='cuda:0', requires_grad=True)
2022-03-23 12:58:29.415809
Epoch: [16][0/15], lr: 0.00050	Time 3.200 (3.200)	Data 2.476 (2.476)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9731], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1278], device='cuda:0', requires_grad=True)
2022-03-23 12:58:41.011066
Epoch: [17][0/15], lr: 0.00050	Time 3.608 (3.608)	Data 3.031 (3.031)	Loss 0.0005 (0.0005)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9731], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1275], device='cuda:0', requires_grad=True)
2022-03-23 12:58:51.856516
Epoch: [18][0/15], lr: 0.00050	Time 2.692 (2.692)	Data 1.925 (1.925)	Loss 0.0003 (0.0003)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9729], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1272], device='cuda:0', requires_grad=True)
2022-03-23 12:59:02.882075
Epoch: [19][0/15], lr: 0.00050	Time 2.889 (2.889)	Data 2.204 (2.204)	Loss 0.0011 (0.0011)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9733], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1272], device='cuda:0', requires_grad=True)
Phase 5 : Eval RGB Model for the Tasks Trained so far
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Load the Trained Model from checkpoint/ucf101/51/2/005/task_024.pth.tar
exemplar : 495
Computing the class mean vectors...
Eval Task 0 for Age 24
Current Task : [37, 97, 56, 55, 33, 84, 3, 4, 72, 59, 66, 48, 65, 91, 99, 39, 34, 22, 67, 74, 19, 35, 9, 86, 88, 63, 85, 38, 54, 25, 57, 62, 83, 76, 6, 13, 2, 53, 8, 24, 44, 12, 100, 29, 5, 17, 15, 73, 47, 27, 46]
video number : 1909
video number + exemplar : 1909
DataLoader Constructed
Test: [0/120]	Time 4.724 (4.724)	Prec@1 62.500 (62.500)
Test: [100/120]	Time 0.578 (0.548)	Prec@1 56.250 (56.869)
Testing Results: Prec@1 57.622
Classify using the NME Classifier...
Test (NME): [0/120]	Time 0.000 (0.000)	Prec@1 75.000 (75.000)
Test (NME): [100/120]	Time 0.000 (0.000)	Prec@1 62.500 (65.780)
Testing Results (NME): Prec@1 66.317
Eval Task 1 for Age 24
Current Task : [98, 96]
video number : 70
video number + exemplar : 70
DataLoader Constructed
Test: [0/5]	Time 4.239 (4.239)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 55.714
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 62.500 (62.500)
Testing Results (NME): Prec@1 70.000
Eval Task 2 for Age 24
Current Task : [18, 90]
video number : 56
video number + exemplar : 56
DataLoader Constructed
Test: [0/4]	Time 3.427 (3.427)	Prec@1 37.500 (37.500)
Testing Results: Prec@1 62.500
Classify using the NME Classifier...
Test (NME): [0/4]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 69.643
Eval Task 3 for Age 24
Current Task : [75, 31]
video number : 73
video number + exemplar : 73
DataLoader Constructed
Test: [0/5]	Time 4.001 (4.001)	Prec@1 81.250 (81.250)
Testing Results: Prec@1 73.973
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 63.014
Eval Task 4 for Age 24
Current Task : [95, 49]
video number : 62
video number + exemplar : 62
DataLoader Constructed
Test: [0/4]	Time 4.044 (4.044)	Prec@1 75.000 (75.000)
Testing Results: Prec@1 77.419
Classify using the NME Classifier...
Test (NME): [0/4]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 87.097
Eval Task 5 for Age 24
Current Task : [43, 78]
video number : 92
video number + exemplar : 92
DataLoader Constructed
Test: [0/6]	Time 3.788 (3.788)	Prec@1 56.250 (56.250)
Testing Results: Prec@1 59.783
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 70.652
Eval Task 6 for Age 24
Current Task : [23, 68]
video number : 84
video number + exemplar : 84
DataLoader Constructed
Test: [0/6]	Time 4.261 (4.261)	Prec@1 43.750 (43.750)
Testing Results: Prec@1 60.714
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 43.750 (43.750)
Testing Results (NME): Prec@1 46.429
Eval Task 7 for Age 24
Current Task : [16, 7]
video number : 84
video number + exemplar : 84
DataLoader Constructed
Test: [0/6]	Time 3.841 (3.841)	Prec@1 12.500 (12.500)
Testing Results: Prec@1 35.714
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 12.500 (12.500)
Testing Results (NME): Prec@1 28.571
Eval Task 8 for Age 24
Current Task : [26, 21]
video number : 84
video number + exemplar : 84
DataLoader Constructed
Test: [0/6]	Time 3.648 (3.648)	Prec@1 81.250 (81.250)
Testing Results: Prec@1 73.810
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 93.750 (93.750)
Testing Results (NME): Prec@1 84.524
Eval Task 9 for Age 24
Current Task : [50, 70]
video number : 78
video number + exemplar : 78
DataLoader Constructed
Test: [0/5]	Time 3.775 (3.775)	Prec@1 75.000 (75.000)
Testing Results: Prec@1 75.641
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 70.513
Eval Task 10 for Age 24
Current Task : [32, 52]
video number : 72
video number + exemplar : 72
DataLoader Constructed
Test: [0/5]	Time 4.427 (4.427)	Prec@1 56.250 (56.250)
Testing Results: Prec@1 59.722
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 81.944
Eval Task 11 for Age 24
Current Task : [11, 69]
video number : 68
video number + exemplar : 68
DataLoader Constructed
Test: [0/5]	Time 3.980 (3.980)	Prec@1 87.500 (87.500)
Testing Results: Prec@1 83.824
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 87.500 (87.500)
Testing Results (NME): Prec@1 82.353
Eval Task 12 for Age 24
Current Task : [93, 14]
video number : 62
video number + exemplar : 62
DataLoader Constructed
Test: [0/4]	Time 4.063 (4.063)	Prec@1 25.000 (25.000)
Testing Results: Prec@1 45.161
Classify using the NME Classifier...
Test (NME): [0/4]	Time 0.000 (0.000)	Prec@1 37.500 (37.500)
Testing Results (NME): Prec@1 53.226
Eval Task 13 for Age 24
Current Task : [79, 10]
video number : 70
video number + exemplar : 70
DataLoader Constructed
Test: [0/5]	Time 4.060 (4.060)	Prec@1 81.250 (81.250)
Testing Results: Prec@1 84.286
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 74.286
Eval Task 14 for Age 24
Current Task : [80, 77]
video number : 83
video number + exemplar : 83
DataLoader Constructed
Test: [0/6]	Time 4.758 (4.758)	Prec@1 56.250 (56.250)
Testing Results: Prec@1 68.675
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 43.750 (43.750)
Testing Results (NME): Prec@1 61.446
Eval Task 15 for Age 24
Current Task : [81, 28]
video number : 68
video number + exemplar : 68
DataLoader Constructed
Test: [0/5]	Time 3.833 (3.833)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 75.000
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 63.235
Eval Task 16 for Age 24
Current Task : [82, 30]
video number : 68
video number + exemplar : 68
DataLoader Constructed
Test: [0/5]	Time 3.960 (3.960)	Prec@1 93.750 (93.750)
Testing Results: Prec@1 95.588
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 87.500 (87.500)
Testing Results (NME): Prec@1 85.294
Eval Task 17 for Age 24
Current Task : [20, 41]
video number : 82
video number + exemplar : 82
DataLoader Constructed
Test: [0/6]	Time 4.174 (4.174)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 79.268
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 62.500 (62.500)
Testing Results (NME): Prec@1 76.829
Eval Task 18 for Age 24
Current Task : [58, 42]
video number : 78
video number + exemplar : 78
DataLoader Constructed
Test: [0/5]	Time 4.116 (4.116)	Prec@1 43.750 (43.750)
Testing Results: Prec@1 44.872
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 57.692
Eval Task 19 for Age 24
Current Task : [60, 36]
video number : 77
video number + exemplar : 77
DataLoader Constructed
Test: [0/5]	Time 3.903 (3.903)	Prec@1 56.250 (56.250)
Testing Results: Prec@1 77.922
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 74.026
Eval Task 20 for Age 24
Current Task : [40, 45]
video number : 75
video number + exemplar : 75
DataLoader Constructed
Test: [0/5]	Time 4.247 (4.247)	Prec@1 87.500 (87.500)
Testing Results: Prec@1 86.667
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 87.500 (87.500)
Testing Results (NME): Prec@1 81.333
Eval Task 21 for Age 24
Current Task : [89, 0]
video number : 83
video number + exemplar : 83
DataLoader Constructed
Test: [0/6]	Time 4.412 (4.412)	Prec@1 50.000 (50.000)
Testing Results: Prec@1 46.988
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 59.036
Eval Task 22 for Age 24
Current Task : [61, 1]
video number : 80
video number + exemplar : 80
DataLoader Constructed
Test: [0/5]	Time 4.155 (4.155)	Prec@1 81.250 (81.250)
Testing Results: Prec@1 85.000
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 85.000
Eval Task 23 for Age 24
Current Task : [92, 94]
video number : 81
video number + exemplar : 81
DataLoader Constructed
Test: [0/6]	Time 4.404 (4.404)	Prec@1 100.000 (100.000)
Testing Results: Prec@1 92.593
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 93.750 (93.750)
Testing Results (NME): Prec@1 79.012
Eval Task 24 for Age 24
Current Task : [64, 71]
video number : 74
video number + exemplar : 74
DataLoader Constructed
Test: [0/5]	Time 4.600 (4.600)	Prec@1 100.000 (100.000)
Testing Results: Prec@1 95.946
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 87.500 (87.500)
Testing Results (NME): Prec@1 83.784
num_test_videos [1909, 70, 56, 73, 62, 92, 84, 84, 84, 78, 72, 68, 62, 70, 83, 68, 68, 82, 78, 77, 75, 83, 80, 81, 74]
Method : OURS
----AGE 25----
current_task  [87, 51]
current_head  101
Phase 2 : Train RGB Model in an Incremental Manner
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Load the Previous Model
Copy the old Model
lambda_0  : [1.0, 0.07035623639735145]
Increment the Model
SplitCosineLinear(
  input_features=512, output_features=303, sigma=tensor([3.9733]), eta=tensor([3.1272])
  (fc1): CosineLinear(input_features=512, output_features=297, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
video number : 183
video number + exemplar : 678
DataLoader Constructed : Train 21
Optimizer Constructed
video number : 183
video number + exemplar : 183
Initialize Cosine Classifier
Computing the class mean vectors...
2022-03-23 13:06:40.674355
Epoch: [0][0/21], lr: 0.00100	Time 4.187 (4.187)	Data 2.521 (2.521)	Loss 0.1890 (0.1890)	Loss CE 0.1324 (0.1324)	Loss KD (Logit) 0.0019 (0.0019)	Loss KD (GCAM) 0.0002 (0.0002)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5641 (0.5641)	Loss REG 0.0000 (0.0000)	Prec@1 96.875 (96.875)
/home/ustc/anaconda3/envs/lhc/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Sigma : Parameter containing:
tensor([3.9688], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1226], device='cuda:0', requires_grad=True)
2022-03-23 13:07:02.222822
Epoch: [1][0/21], lr: 0.00100	Time 3.882 (3.882)	Data 2.575 (2.575)	Loss 0.0699 (0.0699)	Loss CE 0.0157 (0.0157)	Loss KD (Logit) 0.0020 (0.0020)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5402 (0.5402)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9592], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1155], device='cuda:0', requires_grad=True)
2022-03-23 13:07:27.314303
Epoch: [2][0/21], lr: 0.00100	Time 4.476 (4.476)	Data 3.366 (3.366)	Loss 0.1696 (0.1696)	Loss CE 0.1224 (0.1224)	Loss KD (Logit) 0.0020 (0.0020)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.4698 (0.4698)	Loss REG 0.0000 (0.0000)	Prec@1 93.750 (93.750)
Sigma : Parameter containing:
tensor([3.9591], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1147], device='cuda:0', requires_grad=True)
2022-03-23 13:07:52.264463
Epoch: [3][0/21], lr: 0.00100	Time 4.161 (4.161)	Data 3.048 (3.048)	Loss 0.1411 (0.1411)	Loss CE 0.0862 (0.0862)	Loss KD (Logit) 0.0020 (0.0020)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5466 (0.5466)	Loss REG 0.0000 (0.0000)	Prec@1 96.875 (96.875)
Sigma : Parameter containing:
tensor([3.9607], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1158], device='cuda:0', requires_grad=True)
2022-03-23 13:08:17.274210
Epoch: [4][0/21], lr: 0.00100	Time 3.925 (3.925)	Data 2.570 (2.570)	Loss 0.2920 (0.2920)	Loss CE 0.2349 (0.2349)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5686 (0.5686)	Loss REG 0.0000 (0.0000)	Prec@1 96.875 (96.875)
Sigma : Parameter containing:
tensor([3.9581], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1148], device='cuda:0', requires_grad=True)
2022-03-23 13:08:40.800518
Epoch: [5][0/21], lr: 0.00100	Time 3.850 (3.850)	Data 2.609 (2.609)	Loss 0.0693 (0.0693)	Loss CE 0.0149 (0.0149)	Loss KD (Logit) 0.0020 (0.0020)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5412 (0.5412)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9638], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1186], device='cuda:0', requires_grad=True)
2022-03-23 13:09:03.642545
Epoch: [6][0/21], lr: 0.00100	Time 3.718 (3.718)	Data 2.611 (2.611)	Loss 0.0553 (0.0553)	Loss CE 0.0006 (0.0006)	Loss KD (Logit) 0.0020 (0.0020)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5446 (0.5446)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9647], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1199], device='cuda:0', requires_grad=True)
2022-03-23 13:09:26.106978
Epoch: [7][0/21], lr: 0.00100	Time 3.665 (3.665)	Data 2.922 (2.922)	Loss 0.1442 (0.1442)	Loss CE 0.0908 (0.0908)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5316 (0.5316)	Loss REG 0.0000 (0.0000)	Prec@1 96.875 (96.875)
Sigma : Parameter containing:
tensor([3.9676], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1225], device='cuda:0', requires_grad=True)
2022-03-23 13:09:48.690302
Epoch: [8][0/21], lr: 0.00100	Time 4.019 (4.019)	Data 2.857 (2.857)	Loss 0.1626 (0.1626)	Loss CE 0.1076 (0.1076)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5469 (0.5469)	Loss REG 0.0000 (0.0000)	Prec@1 93.750 (93.750)
Sigma : Parameter containing:
tensor([3.9640], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1207], device='cuda:0', requires_grad=True)
2022-03-23 13:10:13.131721
Epoch: [9][0/21], lr: 0.00100	Time 3.706 (3.706)	Data 2.201 (2.201)	Loss 0.0672 (0.0672)	Loss CE 0.0084 (0.0084)	Loss KD (Logit) 0.0020 (0.0020)	Loss KD (GCAM) 0.0003 (0.0003)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5855 (0.5855)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9704], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1246], device='cuda:0', requires_grad=True)
2022-03-23 13:10:37.827540
Epoch: [10][0/21], lr: 0.00100	Time 3.716 (3.716)	Data 2.517 (2.517)	Loss 0.1367 (0.1367)	Loss CE 0.0767 (0.0767)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5976 (0.5976)	Loss REG 0.0000 (0.0000)	Prec@1 96.875 (96.875)
Sigma : Parameter containing:
tensor([3.9736], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1270], device='cuda:0', requires_grad=True)
2022-03-23 13:11:02.671539
Epoch: [11][0/21], lr: 0.00100	Time 3.817 (3.817)	Data 2.532 (2.532)	Loss 0.0616 (0.0616)	Loss CE 0.0050 (0.0050)	Loss KD (Logit) 0.0022 (0.0022)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5634 (0.5634)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9713], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1253], device='cuda:0', requires_grad=True)
2022-03-23 13:11:24.453084
Epoch: [12][0/21], lr: 0.00100	Time 3.548 (3.548)	Data 2.533 (2.533)	Loss 0.0631 (0.0631)	Loss CE 0.0079 (0.0079)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5492 (0.5492)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9695], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1242], device='cuda:0', requires_grad=True)
2022-03-23 13:11:46.506691
Epoch: [13][0/21], lr: 0.00100	Time 3.688 (3.688)	Data 2.455 (2.455)	Loss 0.0561 (0.0561)	Loss CE 0.0005 (0.0005)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5540 (0.5540)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9703], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1250], device='cuda:0', requires_grad=True)
2022-03-23 13:12:07.756418
Epoch: [14][0/21], lr: 0.00100	Time 3.555 (3.555)	Data 2.387 (2.387)	Loss 0.0548 (0.0548)	Loss CE 0.0012 (0.0012)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5331 (0.5331)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9749], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1277], device='cuda:0', requires_grad=True)
2022-03-23 13:12:29.349697
Epoch: [15][0/21], lr: 0.00100	Time 3.787 (3.787)	Data 2.725 (2.725)	Loss 0.0655 (0.0655)	Loss CE 0.0094 (0.0094)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5580 (0.5580)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9792], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1300], device='cuda:0', requires_grad=True)
2022-03-23 13:12:50.832637
Epoch: [16][0/21], lr: 0.00100	Time 3.804 (3.804)	Data 2.390 (2.390)	Loss 0.0543 (0.0543)	Loss CE 0.0007 (0.0007)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5336 (0.5336)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9816], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1311], device='cuda:0', requires_grad=True)
2022-03-23 13:13:12.045814
Epoch: [17][0/21], lr: 0.00100	Time 3.811 (3.811)	Data 2.710 (2.710)	Loss 0.0530 (0.0530)	Loss CE 0.0021 (0.0021)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5054 (0.5054)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9857], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1337], device='cuda:0', requires_grad=True)
2022-03-23 13:13:33.172642
Epoch: [18][0/21], lr: 0.00100	Time 3.822 (3.822)	Data 2.678 (2.678)	Loss 0.0567 (0.0567)	Loss CE 0.0011 (0.0011)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5532 (0.5532)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9909], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1367], device='cuda:0', requires_grad=True)
2022-03-23 13:13:53.846389
Epoch: [19][0/21], lr: 0.00100	Time 3.597 (3.597)	Data 2.381 (2.381)	Loss 0.0596 (0.0596)	Loss CE 0.0035 (0.0035)	Loss KD (Logit) 0.0022 (0.0022)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5581 (0.5581)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9906], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1362], device='cuda:0', requires_grad=True)
2022-03-23 13:14:14.104940
Epoch: [20][0/21], lr: 0.00010	Time 3.410 (3.410)	Data 2.239 (2.239)	Loss 0.0667 (0.0667)	Loss CE 0.0054 (0.0054)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6105 (0.6105)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9901], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1360], device='cuda:0', requires_grad=True)
2022-03-23 13:14:34.734862
Epoch: [21][0/21], lr: 0.00010	Time 3.818 (3.818)	Data 2.807 (2.807)	Loss 0.0590 (0.0590)	Loss CE 0.0002 (0.0002)	Loss KD (Logit) 0.0022 (0.0022)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5847 (0.5847)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9900], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1359], device='cuda:0', requires_grad=True)
2022-03-23 13:14:56.057184
Epoch: [22][0/21], lr: 0.00010	Time 3.486 (3.486)	Data 2.754 (2.754)	Loss 0.0582 (0.0582)	Loss CE 0.0005 (0.0005)	Loss KD (Logit) 0.0022 (0.0022)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5744 (0.5744)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9900], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1359], device='cuda:0', requires_grad=True)
2022-03-23 13:15:14.497426
Epoch: [23][0/21], lr: 0.00010	Time 3.333 (3.333)	Data 2.345 (2.345)	Loss 0.0591 (0.0591)	Loss CE 0.0007 (0.0007)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5806 (0.5806)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9900], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1359], device='cuda:0', requires_grad=True)
2022-03-23 13:15:32.766544
Epoch: [24][0/21], lr: 0.00010	Time 3.283 (3.283)	Data 2.331 (2.331)	Loss 0.0527 (0.0527)	Loss CE 0.0006 (0.0006)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5185 (0.5185)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9900], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1360], device='cuda:0', requires_grad=True)
2022-03-23 13:15:51.022940
Epoch: [25][0/21], lr: 0.00010	Time 3.550 (3.550)	Data 2.083 (2.083)	Loss 0.0563 (0.0563)	Loss CE 0.0021 (0.0021)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5392 (0.5392)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9902], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1361], device='cuda:0', requires_grad=True)
2022-03-23 13:16:09.350468
Epoch: [26][0/21], lr: 0.00010	Time 3.354 (3.354)	Data 2.003 (2.003)	Loss 0.0582 (0.0582)	Loss CE 0.0034 (0.0034)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5450 (0.5450)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9903], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1361], device='cuda:0', requires_grad=True)
2022-03-23 13:16:27.824371
Epoch: [27][0/21], lr: 0.00010	Time 2.948 (2.948)	Data 2.015 (2.015)	Loss 0.0590 (0.0590)	Loss CE 0.0008 (0.0008)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5786 (0.5786)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9904], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1361], device='cuda:0', requires_grad=True)
2022-03-23 13:16:47.284378
Epoch: [28][0/21], lr: 0.00010	Time 3.367 (3.367)	Data 2.230 (2.230)	Loss 0.0635 (0.0635)	Loss CE 0.0042 (0.0042)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5910 (0.5910)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9907], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1363], device='cuda:0', requires_grad=True)
2022-03-23 13:17:06.668438
Epoch: [29][0/21], lr: 0.00010	Time 3.620 (3.620)	Data 2.841 (2.841)	Loss 0.0569 (0.0569)	Loss CE 0.0005 (0.0005)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5614 (0.5614)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9908], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:17:25.743155
Epoch: [30][0/21], lr: 0.00001	Time 3.492 (3.492)	Data 1.978 (1.978)	Loss 0.0561 (0.0561)	Loss CE 0.0003 (0.0003)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5551 (0.5551)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9908], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:17:44.903351
Epoch: [31][0/21], lr: 0.00001	Time 3.334 (3.334)	Data 2.065 (2.065)	Loss 0.0534 (0.0534)	Loss CE 0.0019 (0.0019)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5116 (0.5116)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9908], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:18:03.783943
Epoch: [32][0/21], lr: 0.00001	Time 3.437 (3.437)	Data 2.491 (2.491)	Loss 0.0560 (0.0560)	Loss CE 0.0041 (0.0041)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5166 (0.5166)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9908], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:18:23.941214
Epoch: [33][0/21], lr: 0.00001	Time 4.301 (4.301)	Data 3.184 (3.184)	Loss 0.0591 (0.0591)	Loss CE 0.0003 (0.0003)	Loss KD (Logit) 0.0020 (0.0020)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5856 (0.5856)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9909], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:18:42.659435
Epoch: [34][0/21], lr: 0.00001	Time 3.547 (3.547)	Data 2.723 (2.723)	Loss 0.0529 (0.0529)	Loss CE 0.0008 (0.0008)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5179 (0.5179)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9909], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:19:01.234764
Epoch: [35][0/21], lr: 0.00001	Time 3.298 (3.298)	Data 2.280 (2.280)	Loss 0.0577 (0.0577)	Loss CE 0.0011 (0.0011)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5631 (0.5631)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9909], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:19:19.346090
Epoch: [36][0/21], lr: 0.00001	Time 3.295 (3.295)	Data 2.254 (2.254)	Loss 0.0585 (0.0585)	Loss CE 0.0034 (0.0034)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5487 (0.5487)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9909], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:19:38.010428
Epoch: [37][0/21], lr: 0.00001	Time 3.266 (3.266)	Data 2.171 (2.171)	Loss 0.0567 (0.0567)	Loss CE 0.0006 (0.0006)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5592 (0.5592)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9909], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:19:56.619531
Epoch: [38][0/21], lr: 0.00001	Time 3.427 (3.427)	Data 2.414 (2.414)	Loss 0.0545 (0.0545)	Loss CE 0.0003 (0.0003)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5396 (0.5396)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9909], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:20:15.096558
Epoch: [39][0/21], lr: 0.00001	Time 3.348 (3.348)	Data 2.136 (2.136)	Loss 0.0576 (0.0576)	Loss CE 0.0008 (0.0008)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5650 (0.5650)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9909], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:20:33.373465
Epoch: [40][0/21], lr: 0.00001	Time 3.224 (3.224)	Data 2.146 (2.146)	Loss 0.0549 (0.0549)	Loss CE 0.0004 (0.0004)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5431 (0.5431)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9910], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:20:51.691194
Epoch: [41][0/21], lr: 0.00001	Time 3.442 (3.442)	Data 2.354 (2.354)	Loss 0.0527 (0.0527)	Loss CE 0.0006 (0.0006)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5181 (0.5181)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9909], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:21:10.010621
Epoch: [42][0/21], lr: 0.00001	Time 3.418 (3.418)	Data 2.209 (2.209)	Loss 0.0822 (0.0822)	Loss CE 0.0265 (0.0265)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5541 (0.5541)	Loss REG 0.0000 (0.0000)	Prec@1 96.875 (96.875)
Sigma : Parameter containing:
tensor([3.9909], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:21:28.225428
Epoch: [43][0/21], lr: 0.00001	Time 3.196 (3.196)	Data 2.234 (2.234)	Loss 0.0615 (0.0615)	Loss CE 0.0017 (0.0017)	Loss KD (Logit) 0.0020 (0.0020)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5957 (0.5957)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9909], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:21:46.697294
Epoch: [44][0/21], lr: 0.00001	Time 3.294 (3.294)	Data 2.244 (2.244)	Loss 0.0555 (0.0555)	Loss CE 0.0004 (0.0004)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5485 (0.5485)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9910], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:22:05.393617
Epoch: [45][0/21], lr: 0.00001	Time 3.382 (3.382)	Data 2.552 (2.552)	Loss 0.0633 (0.0633)	Loss CE 0.0077 (0.0077)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5533 (0.5533)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9910], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:22:24.195337
Epoch: [46][0/21], lr: 0.00001	Time 3.775 (3.775)	Data 2.909 (2.909)	Loss 0.0547 (0.0547)	Loss CE 0.0004 (0.0004)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5398 (0.5398)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9910], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:22:42.669586
Epoch: [47][0/21], lr: 0.00001	Time 3.287 (3.287)	Data 2.196 (2.196)	Loss 0.0651 (0.0651)	Loss CE 0.0124 (0.0124)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5242 (0.5242)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9910], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:23:00.961649
Epoch: [48][0/21], lr: 0.00001	Time 3.218 (3.218)	Data 2.105 (2.105)	Loss 0.0556 (0.0556)	Loss CE 0.0005 (0.0005)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5484 (0.5484)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9910], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
2022-03-23 13:23:19.137863
Epoch: [49][0/21], lr: 0.00001	Time 3.297 (3.297)	Data 2.196 (2.196)	Loss 0.0549 (0.0549)	Loss CE 0.0008 (0.0008)	Loss KD (Logit) 0.0021 (0.0021)	Loss KD (GCAM) 0.0004 (0.0004)	Loss MR 0.0000 (0.0000)	Loss DIV 0.5390 (0.5390)	Loss REG 0.0000 (0.0000)	Prec@1 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9910], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1364], device='cuda:0', requires_grad=True)
Update Importance Mask...
Parameter containing:
tensor([0.0942], device='cuda:0', requires_grad=True) Parameter containing:
tensor([0.1883], device='cuda:0', requires_grad=True) Parameter containing:
tensor([0.2827], device='cuda:0', requires_grad=True) Parameter containing:
tensor([0.3766], device='cuda:0', requires_grad=True) Phase 3 : Manage Exemplar Sets
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Construct Exemplar Set
Load the Model
SplitCosineLinear(
  input_features=512, output_features=303, sigma=tensor([3.9910]), eta=tensor([3.1364])
  (fc1): CosineLinear(input_features=512, output_features=297, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
Exemplar per class : 5
video number : 183
video number + exemplar : 183
Phase 4 : Class-balanced Fine-Tuning
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Load the Model
SplitCosineLinear(
  input_features=512, output_features=303, sigma=tensor([3.9910]), eta=tensor([3.1364])
  (fc1): CosineLinear(input_features=512, output_features=297, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
exemplar : 505
DataLoader CBF Constructed : Train 15
Optimizer Constructed
2022-03-23 13:23:57.531104
Epoch: [0][0/15], lr: 0.00050	Time 2.932 (2.932)	Data 2.369 (2.369)	Loss 0.0001 (0.0001)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9913], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1365], device='cuda:0', requires_grad=True)
2022-03-23 13:24:08.274807
Epoch: [1][0/15], lr: 0.00050	Time 2.998 (2.998)	Data 2.512 (2.512)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9915], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1365], device='cuda:0', requires_grad=True)
2022-03-23 13:24:19.062909
Epoch: [2][0/15], lr: 0.00050	Time 2.975 (2.975)	Data 1.948 (1.948)	Loss 0.0024 (0.0024)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9914], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1361], device='cuda:0', requires_grad=True)
2022-03-23 13:24:30.092728
Epoch: [3][0/15], lr: 0.00050	Time 3.297 (3.297)	Data 2.423 (2.423)	Loss 0.0001 (0.0001)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9912], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1358], device='cuda:0', requires_grad=True)
2022-03-23 13:24:40.458766
Epoch: [4][0/15], lr: 0.00050	Time 2.844 (2.844)	Data 1.864 (1.864)	Loss 0.0007 (0.0007)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9911], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1356], device='cuda:0', requires_grad=True)
2022-03-23 13:24:51.074697
Epoch: [5][0/15], lr: 0.00050	Time 2.915 (2.915)	Data 2.331 (2.331)	Loss 0.0005 (0.0005)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9911], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1353], device='cuda:0', requires_grad=True)
2022-03-23 13:25:01.502789
Epoch: [6][0/15], lr: 0.00050	Time 2.849 (2.849)	Data 2.052 (2.052)	Loss 0.0005 (0.0005)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9910], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1350], device='cuda:0', requires_grad=True)
2022-03-23 13:25:12.201426
Epoch: [7][0/15], lr: 0.00050	Time 3.142 (3.142)	Data 2.530 (2.530)	Loss 0.0002 (0.0002)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9913], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1350], device='cuda:0', requires_grad=True)
2022-03-23 13:25:22.947055
Epoch: [8][0/15], lr: 0.00050	Time 2.981 (2.981)	Data 2.258 (2.258)	Loss 0.0003 (0.0003)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9914], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1348], device='cuda:0', requires_grad=True)
2022-03-23 13:25:33.559671
Epoch: [9][0/15], lr: 0.00050	Time 2.986 (2.986)	Data 2.089 (2.089)	Loss 0.0000 (0.0000)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9916], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1348], device='cuda:0', requires_grad=True)
2022-03-23 13:25:44.452397
Epoch: [10][0/15], lr: 0.00050	Time 3.137 (3.137)	Data 2.106 (2.106)	Loss 0.0001 (0.0001)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9918], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1348], device='cuda:0', requires_grad=True)
2022-03-23 13:25:55.170469
Epoch: [11][0/15], lr: 0.00050	Time 2.961 (2.961)	Data 2.151 (2.151)	Loss 0.0003 (0.0003)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9920], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1347], device='cuda:0', requires_grad=True)
2022-03-23 13:26:05.958319
Epoch: [12][0/15], lr: 0.00050	Time 3.010 (3.010)	Data 2.431 (2.431)	Loss 0.0003 (0.0003)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9916], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1343], device='cuda:0', requires_grad=True)
2022-03-23 13:26:16.373408
Epoch: [13][0/15], lr: 0.00050	Time 2.738 (2.738)	Data 1.807 (1.807)	Loss 0.0003 (0.0003)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9910], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1338], device='cuda:0', requires_grad=True)
2022-03-23 13:26:27.309563
Epoch: [14][0/15], lr: 0.00050	Time 3.013 (3.013)	Data 2.313 (2.313)	Loss 0.0001 (0.0001)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9907], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1334], device='cuda:0', requires_grad=True)
2022-03-23 13:26:38.306704
Epoch: [15][0/15], lr: 0.00050	Time 3.434 (3.434)	Data 2.460 (2.460)	Loss 0.0000 (0.0000)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9903], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1329], device='cuda:0', requires_grad=True)
2022-03-23 13:26:49.162078
Epoch: [16][0/15], lr: 0.00050	Time 2.991 (2.991)	Data 2.337 (2.337)	Loss 0.0013 (0.0013)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9903], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1327], device='cuda:0', requires_grad=True)
2022-03-23 13:26:59.723914
Epoch: [17][0/15], lr: 0.00050	Time 3.106 (3.106)	Data 1.997 (1.997)	Loss 0.0006 (0.0006)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9902], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1324], device='cuda:0', requires_grad=True)
2022-03-23 13:27:10.627142
Epoch: [18][0/15], lr: 0.00050	Time 3.076 (3.076)	Data 1.972 (1.972)	Loss 0.0004 (0.0004)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9898], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1319], device='cuda:0', requires_grad=True)
2022-03-23 13:27:21.246430
Epoch: [19][0/15], lr: 0.00050	Time 2.892 (2.892)	Data 2.373 (2.373)	Loss 0.0001 (0.0001)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Sigma : Parameter containing:
tensor([3.9898], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([3.1317], device='cuda:0', requires_grad=True)
Phase 5 : Eval RGB Model for the Tasks Trained so far
=> base model: resnet34
----------------------resnet34 pretrained----------------------
Load the Trained Model from checkpoint/ucf101/51/2/005/task_025.pth.tar
exemplar : 505
Computing the class mean vectors...
Eval Task 0 for Age 25
Current Task : [37, 97, 56, 55, 33, 84, 3, 4, 72, 59, 66, 48, 65, 91, 99, 39, 34, 22, 67, 74, 19, 35, 9, 86, 88, 63, 85, 38, 54, 25, 57, 62, 83, 76, 6, 13, 2, 53, 8, 24, 44, 12, 100, 29, 5, 17, 15, 73, 47, 27, 46]
video number : 1909
video number + exemplar : 1909
DataLoader Constructed
Test: [0/120]	Time 4.589 (4.589)	Prec@1 62.500 (62.500)
Test: [100/120]	Time 0.513 (0.528)	Prec@1 62.500 (57.735)
Testing Results: Prec@1 58.355
Classify using the NME Classifier...
Test (NME): [0/120]	Time 0.000 (0.000)	Prec@1 62.500 (62.500)
Test (NME): [100/120]	Time 0.000 (0.000)	Prec@1 62.500 (66.399)
Testing Results (NME): Prec@1 67.103
Eval Task 1 for Age 25
Current Task : [98, 96]
video number : 70
video number + exemplar : 70
DataLoader Constructed
Test: [0/5]	Time 3.933 (3.933)	Prec@1 50.000 (50.000)
Testing Results: Prec@1 47.143
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 64.286
Eval Task 2 for Age 25
Current Task : [18, 90]
video number : 56
video number + exemplar : 56
DataLoader Constructed
Test: [0/4]	Time 3.609 (3.609)	Prec@1 50.000 (50.000)
Testing Results: Prec@1 57.143
Classify using the NME Classifier...
Test (NME): [0/4]	Time 0.000 (0.000)	Prec@1 50.000 (50.000)
Testing Results (NME): Prec@1 58.929
Eval Task 3 for Age 25
Current Task : [75, 31]
video number : 73
video number + exemplar : 73
DataLoader Constructed
Test: [0/5]	Time 3.836 (3.836)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 69.863
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 79.452
Eval Task 4 for Age 25
Current Task : [95, 49]
video number : 62
video number + exemplar : 62
DataLoader Constructed
Test: [0/4]	Time 3.539 (3.539)	Prec@1 68.750 (68.750)
Testing Results: Prec@1 70.968
Classify using the NME Classifier...
Test (NME): [0/4]	Time 0.000 (0.000)	Prec@1 75.000 (75.000)
Testing Results (NME): Prec@1 79.032
Eval Task 5 for Age 25
Current Task : [43, 78]
video number : 92
video number + exemplar : 92
DataLoader Constructed
Test: [0/6]	Time 4.097 (4.097)	Prec@1 50.000 (50.000)
Testing Results: Prec@1 57.609
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 76.087
Eval Task 6 for Age 25
Current Task : [23, 68]
video number : 84
video number + exemplar : 84
DataLoader Constructed
Test: [0/6]	Time 3.950 (3.950)	Prec@1 31.250 (31.250)
Testing Results: Prec@1 45.238
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 37.500 (37.500)
Testing Results (NME): Prec@1 50.000
Eval Task 7 for Age 25
Current Task : [16, 7]
video number : 84
video number + exemplar : 84
DataLoader Constructed
Test: [0/6]	Time 4.007 (4.007)	Prec@1 18.750 (18.750)
Testing Results: Prec@1 30.952
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 18.750 (18.750)
Testing Results (NME): Prec@1 32.143
Eval Task 8 for Age 25
Current Task : [26, 21]
video number : 84
video number + exemplar : 84
DataLoader Constructed
Test: [0/6]	Time 4.080 (4.080)	Prec@1 87.500 (87.500)
Testing Results: Prec@1 75.000
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 71.429
Eval Task 9 for Age 25
Current Task : [50, 70]
video number : 78
video number + exemplar : 78
DataLoader Constructed
Test: [0/5]	Time 3.931 (3.931)	Prec@1 75.000 (75.000)
Testing Results: Prec@1 75.641
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 75.000 (75.000)
Testing Results (NME): Prec@1 75.641
Eval Task 10 for Age 25
Current Task : [32, 52]
video number : 72
video number + exemplar : 72
DataLoader Constructed
Test: [0/5]	Time 3.714 (3.714)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 63.889
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 75.000 (75.000)
Testing Results (NME): Prec@1 77.778
Eval Task 11 for Age 25
Current Task : [11, 69]
video number : 68
video number + exemplar : 68
DataLoader Constructed
Test: [0/5]	Time 3.473 (3.473)	Prec@1 68.750 (68.750)
Testing Results: Prec@1 76.471
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 82.353
Eval Task 12 for Age 25
Current Task : [93, 14]
video number : 62
video number + exemplar : 62
DataLoader Constructed
Test: [0/4]	Time 3.375 (3.375)	Prec@1 25.000 (25.000)
Testing Results: Prec@1 29.032
Classify using the NME Classifier...
Test (NME): [0/4]	Time 0.000 (0.000)	Prec@1 37.500 (37.500)
Testing Results (NME): Prec@1 56.452
Eval Task 13 for Age 25
Current Task : [79, 10]
video number : 70
video number + exemplar : 70
DataLoader Constructed
Test: [0/5]	Time 3.762 (3.762)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 65.714
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 74.286
Eval Task 14 for Age 25
Current Task : [80, 77]
video number : 83
video number + exemplar : 83
DataLoader Constructed
Test: [0/6]	Time 4.038 (4.038)	Prec@1 50.000 (50.000)
Testing Results: Prec@1 56.627
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 43.750 (43.750)
Testing Results (NME): Prec@1 56.627
Eval Task 15 for Age 25
Current Task : [81, 28]
video number : 68
video number + exemplar : 68
DataLoader Constructed
Test: [0/5]	Time 3.567 (3.567)	Prec@1 43.750 (43.750)
Testing Results: Prec@1 55.882
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 50.000 (50.000)
Testing Results (NME): Prec@1 66.176
Eval Task 16 for Age 25
Current Task : [82, 30]
video number : 68
video number + exemplar : 68
DataLoader Constructed
Test: [0/5]	Time 3.924 (3.924)	Prec@1 81.250 (81.250)
Testing Results: Prec@1 86.765
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 87.500 (87.500)
Testing Results (NME): Prec@1 86.765
Eval Task 17 for Age 25
Current Task : [20, 41]
video number : 82
video number + exemplar : 82
DataLoader Constructed
Test: [0/6]	Time 3.904 (3.904)	Prec@1 62.500 (62.500)
Testing Results: Prec@1 80.488
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 62.500 (62.500)
Testing Results (NME): Prec@1 80.488
Eval Task 18 for Age 25
Current Task : [58, 42]
video number : 78
video number + exemplar : 78
DataLoader Constructed
Test: [0/5]	Time 3.709 (3.709)	Prec@1 43.750 (43.750)
Testing Results: Prec@1 42.308
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 62.500 (62.500)
Testing Results (NME): Prec@1 61.538
Eval Task 19 for Age 25
Current Task : [60, 36]
video number : 77
video number + exemplar : 77
DataLoader Constructed
Test: [0/5]	Time 3.701 (3.701)	Prec@1 43.750 (43.750)
Testing Results: Prec@1 57.143
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 43.750 (43.750)
Testing Results (NME): Prec@1 64.935
Eval Task 20 for Age 25
Current Task : [40, 45]
video number : 75
video number + exemplar : 75
DataLoader Constructed
Test: [0/5]	Time 3.791 (3.791)	Prec@1 93.750 (93.750)
Testing Results: Prec@1 93.333
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 81.250 (81.250)
Testing Results (NME): Prec@1 86.667
Eval Task 21 for Age 25
Current Task : [89, 0]
video number : 83
video number + exemplar : 83
DataLoader Constructed
Test: [0/6]	Time 3.975 (3.975)	Prec@1 56.250 (56.250)
Testing Results: Prec@1 55.422
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 56.250 (56.250)
Testing Results (NME): Prec@1 59.036
Eval Task 22 for Age 25
Current Task : [61, 1]
video number : 80
video number + exemplar : 80
DataLoader Constructed
Test: [0/5]	Time 3.795 (3.795)	Prec@1 87.500 (87.500)
Testing Results: Prec@1 83.750
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 75.000 (75.000)
Testing Results (NME): Prec@1 83.750
Eval Task 23 for Age 25
Current Task : [92, 94]
video number : 81
video number + exemplar : 81
DataLoader Constructed
Test: [0/6]	Time 4.025 (4.025)	Prec@1 93.750 (93.750)
Testing Results: Prec@1 81.481
Classify using the NME Classifier...
Test (NME): [0/6]	Time 0.000 (0.000)	Prec@1 87.500 (87.500)
Testing Results (NME): Prec@1 70.370
Eval Task 24 for Age 25
Current Task : [64, 71]
video number : 74
video number + exemplar : 74
DataLoader Constructed
Test: [0/5]	Time 3.967 (3.967)	Prec@1 81.250 (81.250)
Testing Results: Prec@1 91.892
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 68.750 (68.750)
Testing Results (NME): Prec@1 82.432
Eval Task 25 for Age 25
Current Task : [87, 51]
video number : 70
video number + exemplar : 70
DataLoader Constructed
Test: [0/5]	Time 5.686 (5.686)	Prec@1 81.250 (81.250)
Testing Results: Prec@1 80.000
Classify using the NME Classifier...
Test (NME): [0/5]	Time 0.000 (0.000)	Prec@1 75.000 (75.000)
Testing Results (NME): Prec@1 61.429
num_test_videos [1909, 70, 56, 73, 62, 92, 84, 84, 84, 78, 72, 68, 62, 70, 83, 68, 68, 82, 78, 77, 75, 83, 80, 81, 74, 70]
n_vids [1909   70   56   73   62   92   84   84   84   78   72   68   62   70
   83   68   68   82   78   77   75   83   80   81   74   70]
n_vids_pad [[1909.]
 [  70.]
 [  56.]
 [  73.]
 [  62.]
 [  92.]
 [  84.]
 [  84.]
 [  84.]
 [  78.]
 [  72.]
 [  68.]
 [  62.]
 [  70.]
 [  83.]
 [  68.]
 [  68.]
 [  82.]
 [  78.]
 [  77.]
 [  75.]
 [  83.]
 [  80.]
 [  81.]
 [  74.]
 [  70.]]
tmp [[162000.         149400.         145800.         142300.
  142900.         138700.         137900.         132700.
  130500.         131500.         128300.         129300.
  121400.         122600.         119600.         122200.
  119400.         119400.         115200.         115900.
  118500.         113900.         112500.         110800.
  110000.         111400.        ]
 [-14000.           7000.           6500.           6500.
    6100.           5299.99996948   5299.99998474   3899.99999237
    3999.99996948   5199.99996948   5299.99998474   5099.99998474
    4299.99998474   4999.99996948   5199.99998474   4700.
    4599.99996948   4400.           4399.99998474   3899.99998474
    4500.           3500.           3800.           3899.99998474
    3899.99998474   3300.        ]
 [-11200.         -11200.           5600.           4200.
    4800.           3900.           3900.           3900.
    4400.           3900.           3400.           4400.
    3800.           3900.           4100.           3900.
    3800.           3400.           3500.           4100.
    3300.           3600.           3600.           3400.
    3500.           3200.        ]
 [-14600.         -14600.         -14600.           5999.99993896
    5299.99997711   5399.99993896   4999.99993896   5999.99993896
    5099.99997711   5299.99993896   5499.99996948   4699.99997711
    5299.99997711   5599.99997711   5299.99997711   4599.99997711
    4299.99997711   4799.99997711   4299.99997711   4699.99997711
    4799.99997711   4699.99997711   4599.99997711   5199.99997711
    5399.99997711   5099.99993896]
 [-12400.         -12400.         -12400.         -12400.
    6200.           6099.99995422   5999.99995422   5700.00001526
    5100.00001526   5699.99995422   5899.99995422   5800.00001526
    5700.00001526   5600.00001526   4800.00001526   5700.00001526
    5000.00001526   5400.00001526   5699.99995422   5599.99995422
    5799.99995422   4899.99996948   5300.00001526   4700.00001526
    4799.99996948   4400.00001526]
 [-18400.         -18400.         -18400.         -18400.
  -18400.           9000.           7299.99996948   6799.99996948
    7299.99993896   6700.           6199.99996948   6399.99996948
    6499.99996948   6399.99998474   5799.99998474   5999.99996948
    5799.99998474   5999.99996948   6599.99998474   6399.99998474
    5999.99998474   5499.99998474   5699.99998474   4600.
    5499.99998474   5299.99998474]
 [-16800.         -16800.         -16800.         -16800.
  -16800.         -16800.           7900.           6800.
    6100.           5900.           5600.           6100.
    5300.           5200.           5200.           3900.
    4400.           4300.           5200.           5200.
    5200.           4000.           5100.           4900.
    5100.           3800.        ]
 [-16800.         -16800.         -16800.         -16800.
  -16800.         -16800.         -16800.           8400.
    6300.           5000.           5800.           4800.
    3100.           4100.           3600.           2400.
    3600.           3100.           2900.           1800.
    3400.           2600.           2800.           2300.
    3000.           2600.        ]
 [-16800.         -16800.         -16800.         -16800.
  -16800.         -16800.         -16800.         -16800.
    8300.           8000.           6800.           6900.
    6600.           7200.           6100.           6600.
    6000.           6600.           7000.           6900.
    6600.           6100.           6900.           6900.
    6200.           6300.        ]
 [-15600.         -15600.         -15600.         -15600.
  -15600.         -15600.         -15600.         -15600.
  -15600.           6900.00001526   5799.99999237   5499.99999237
    5599.99998474   5499.99998474   5400.           5699.99999237
    5000.           5200.           6400.00003052   6599.99996948
    6099.99998474   6400.00003052   6299.99999237   5999.99996948
    5900.00003052   5900.00003052]
 [-14400.         -14400.         -14400.         -14400.
  -14400.         -14400.         -14400.         -14400.
  -14400.         -14400.           6700.           5800.
    5600.           5100.           5300.           5000.
    5000.           4100.           5000.           4600.
    4300.           4000.           4100.           3900.
    4300.           4600.        ]
 [-13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.           6400.
    5700.           5500.           5200.           5600.
    5800.           5500.           5700.           5700.
    5500.           5400.           5400.           5000.
    5700.           5200.        ]
 [-12400.         -12400.         -12400.         -12400.
  -12400.         -12400.         -12400.         -12400.
  -12400.         -12400.         -12400.         -12400.
    5299.99996948   4300.           4300.           3800.
    4300.00003052   3299.99999237   2800.00001526   3500.00001526
    3200.00000763   2700.           3199.99999237   2300.00000381
    2800.00000381   1800.00000381]
 [-14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.           6499.99996948   5699.99996948   5899.99996948
    5899.99996948   5799.99996948   5499.99996948   5599.99998474
    5599.99998474   6300.           6000.           5699.99996948
    5899.99996948   4600.        ]
 [-16600.         -16600.         -16600.         -16600.
  -16600.         -16600.         -16600.         -16600.
  -16600.         -16600.         -16600.         -16600.
  -16600.         -16600.           7600.           6400.
    6200.           6600.           6600.           6600.
    6300.           5099.99999237   5399.99999237   6299.99999237
    5699.99999237   4699.99999237]
 [-13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.           6800.
    6000.           5800.           5900.           6000.
    5300.           5300.           5200.           4500.
    5100.           3800.        ]
 [-13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.         -13600.
    6800.           6600.           6800.           6300.
    6600.           6300.           6300.           6400.
    6500.           5900.        ]
 [-16400.         -16400.         -16400.         -16400.
  -16400.         -16400.         -16400.         -16400.
  -16400.         -16400.         -16400.         -16400.
  -16400.         -16400.         -16400.         -16400.
  -16400.           8100.           7900.           7400.
    7400.           6800.           7000.           6700.
    6500.           6600.        ]
 [-15600.         -15600.         -15600.         -15600.
  -15600.         -15600.         -15600.         -15600.
  -15600.         -15600.         -15600.         -15600.
  -15600.         -15600.         -15600.         -15600.
  -15600.         -15600.           7500.           6299.99998474
    5999.99996948   5599.99996948   5099.99999237   4899.99999237
    3500.00001526   3299.99999619]
 [-15400.         -15400.         -15400.         -15400.
  -15400.         -15400.         -15400.         -15400.
  -15400.         -15400.         -15400.         -15400.
  -15400.         -15400.         -15400.         -15400.
  -15400.         -15400.         -15400.           7200.00001526
    5899.99994659   5899.99994659   5199.99996185   6099.99993134
    5999.99993134   4399.99997711]
 [-15000.         -15000.         -15000.         -15000.
  -15000.         -15000.         -15000.         -15000.
  -15000.         -15000.         -15000.         -15000.
  -15000.         -15000.         -15000.         -15000.
  -15000.         -15000.         -15000.         -15000.
    7000.           6799.99996948   6500.           7000.
    6499.99996948   7000.        ]
 [-16600.         -16600.         -16600.         -16600.
  -16600.         -16600.         -16600.         -16600.
  -16600.         -16600.         -16600.         -16600.
  -16600.         -16600.         -16600.         -16600.
  -16600.         -16600.         -16600.         -16600.
  -16600.           8100.           6699.99999237   5599.99999619
    3899.99999619   4599.99999619]
 [-16000.         -16000.         -16000.         -16000.
  -16000.         -16000.         -16000.         -16000.
  -16000.         -16000.         -16000.         -16000.
  -16000.         -16000.         -16000.         -16000.
  -16000.         -16000.         -16000.         -16000.
  -16000.         -16000.           7500.           7300.
    6800.           6700.        ]
 [-16200.         -16200.         -16200.         -16200.
  -16200.         -16200.         -16200.         -16200.
  -16200.         -16200.         -16200.         -16200.
  -16200.         -16200.         -16200.         -16200.
  -16200.         -16200.         -16200.         -16200.
  -16200.         -16200.         -16200.           8100.
    7500.           6600.        ]
 [-14800.         -14800.         -14800.         -14800.
  -14800.         -14800.         -14800.         -14800.
  -14800.         -14800.         -14800.         -14800.
  -14800.         -14800.         -14800.         -14800.
  -14800.         -14800.         -14800.         -14800.
  -14800.         -14800.         -14800.         -14800.
    7100.           6800.        ]
 [-14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.           5599.99998474]]
tmp [[ 1.26000000e+04  3.60000000e+03  3.50000000e+03 -6.00000000e+02
   4.20000000e+03  8.00000000e+02  5.20000000e+03  2.20000000e+03
  -1.00000000e+03  3.20000000e+03 -1.00000000e+03  7.90000000e+03
  -1.20000000e+03  3.00000000e+03 -2.60000000e+03  2.80000000e+03
  -0.00000000e+00  4.20000000e+03 -7.00000000e+02 -2.60000000e+03
   4.60000000e+03  1.40000000e+03  1.70000000e+03  8.00000000e+02
  -1.40000000e+03]
 [-2.10000000e+04  5.00000000e+02 -0.00000000e+00  4.00000000e+02
   8.00000031e+02 -1.52587900e-05  1.39999999e+03 -9.99999771e+01
  -1.20000000e+03 -1.00000015e+02  2.00000000e+02  8.00000000e+02
  -6.99999985e+02 -2.00000015e+02  4.99999985e+02  1.00000031e+02
   1.99999969e+02  1.52587882e-05  5.00000000e+02 -6.00000015e+02
   1.00000000e+03 -3.00000000e+02 -9.99999847e+01 -0.00000000e+00
   5.99999985e+02]
 [-0.00000000e+00 -1.68000000e+04  1.40000000e+03 -6.00000000e+02
   9.00000000e+02 -0.00000000e+00 -0.00000000e+00 -5.00000000e+02
   5.00000000e+02  5.00000000e+02 -1.00000000e+03  6.00000000e+02
  -1.00000000e+02 -2.00000000e+02  2.00000000e+02  1.00000000e+02
   4.00000000e+02 -1.00000000e+02 -6.00000000e+02  8.00000000e+02
  -3.00000000e+02 -0.00000000e+00  2.00000000e+02 -1.00000000e+02
   3.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -2.05999999e+04  6.99999962e+02
  -9.99999619e+01  4.00000000e+02 -1.00000000e+03  8.99999962e+02
  -1.99999962e+02 -2.00000031e+02  7.99999992e+02 -6.00000000e+02
  -3.00000000e+02  3.00000000e+02  7.00000000e+02  3.00000000e+02
  -5.00000000e+02  5.00000000e+02 -4.00000000e+02 -1.00000000e+02
   1.00000000e+02  1.00000000e+02 -6.00000000e+02 -2.00000000e+02
   3.00000038e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -1.86000000e+04
   1.00000046e+02  1.00000000e+02  2.99999939e+02  6.00000000e+02
  -5.99999939e+02 -2.00000000e+02  9.99999390e+01  1.00000000e+02
   1.00000000e+02  8.00000000e+02 -9.00000000e+02  7.00000000e+02
  -4.00000000e+02 -2.99999939e+02  1.00000000e+02 -2.00000000e+02
   8.99999985e+02 -4.00000046e+02  6.00000000e+02 -9.99999542e+01
   3.99999954e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -2.74000000e+04  1.70000003e+03  5.00000000e+02 -4.99999969e+02
   5.99999939e+02  5.00000031e+02 -2.00000000e+02 -1.00000000e+02
   9.99999847e+01  6.00000000e+02 -1.99999985e+02  1.99999985e+02
  -1.99999985e+02 -6.00000015e+02  2.00000000e+02  4.00000000e+02
   5.00000000e+02 -2.00000000e+02  1.09999998e+03 -8.99999985e+02
   2.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -2.47000000e+04  1.10000000e+03  7.00000000e+02
   2.00000000e+02  3.00000000e+02 -5.00000000e+02  8.00000000e+02
   1.00000000e+02 -0.00000000e+00  1.30000000e+03 -5.00000000e+02
   1.00000000e+02 -9.00000000e+02 -0.00000000e+00 -0.00000000e+00
   1.20000000e+03 -1.10000000e+03  2.00000000e+02 -2.00000000e+02
   1.30000000e+03]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -2.52000000e+04  2.10000000e+03
   1.30000000e+03 -8.00000000e+02  1.00000000e+03  1.70000000e+03
  -1.00000000e+03  5.00000000e+02  1.20000000e+03 -1.20000000e+03
   5.00000000e+02  2.00000000e+02  1.10000000e+03 -1.60000000e+03
   8.00000000e+02 -2.00000000e+02  5.00000000e+02 -7.00000000e+02
   4.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -2.51000000e+04
   3.00000000e+02  1.20000000e+03 -1.00000000e+02  3.00000000e+02
  -6.00000000e+02  1.10000000e+03 -5.00000000e+02  6.00000000e+02
  -6.00000000e+02 -4.00000000e+02  1.00000000e+02  3.00000000e+02
   5.00000000e+02 -8.00000000e+02 -0.00000000e+00  7.00000000e+02
  -1.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -2.25000000e+04  1.10000002e+03  3.00000000e+02 -9.99999924e+01
   1.00000000e+02  9.99999847e+01 -2.99999992e+02  6.99999992e+02
  -2.00000000e+02 -1.20000003e+03 -1.99999939e+02  4.99999985e+02
  -3.00000046e+02  1.00000038e+02  3.00000023e+02  9.99999390e+01
  -0.00000000e+00]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -2.11000000e+04  9.00000000e+02  2.00000000e+02
   5.00000000e+02 -2.00000000e+02  3.00000000e+02 -0.00000000e+00
   9.00000000e+02 -9.00000000e+02  4.00000000e+02  3.00000000e+02
   3.00000000e+02 -1.00000000e+02  2.00000000e+02 -4.00000000e+02
  -3.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -2.00000000e+04  7.00000000e+02
   2.00000000e+02  3.00000000e+02 -4.00000000e+02 -2.00000000e+02
   3.00000000e+02 -2.00000000e+02 -0.00000000e+00  2.00000000e+02
   1.00000000e+02 -0.00000000e+00  4.00000000e+02 -7.00000000e+02
   5.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -1.77000000e+04
   9.99999969e+02 -0.00000000e+00  5.00000000e+02 -5.00000031e+02
   1.00000004e+03  4.99999977e+02 -7.00000000e+02  3.00000008e+02
   5.00000008e+02 -4.99999992e+02  8.99999989e+02 -5.00000000e+02
   1.00000000e+03]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -2.05000000e+04  8.00000000e+02 -2.00000000e+02 -0.00000000e+00
   1.00000000e+02  3.00000000e+02 -1.00000015e+02 -0.00000000e+00
  -7.00000015e+02  3.00000000e+02  3.00000031e+02 -2.00000000e+02
   1.29999997e+03]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -2.42000000e+04  1.20000000e+03  2.00000000e+02
  -4.00000000e+02 -0.00000000e+00 -0.00000000e+00  3.00000000e+02
   1.20000001e+03 -3.00000000e+02 -9.00000000e+02  6.00000000e+02
   1.00000000e+03]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -2.04000000e+04  8.00000000e+02
   2.00000000e+02 -1.00000000e+02 -1.00000000e+02  7.00000000e+02
  -0.00000000e+00  1.00000000e+02  7.00000000e+02 -6.00000000e+02
   1.30000000e+03]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -2.04000000e+04
   2.00000000e+02 -2.00000000e+02  5.00000000e+02 -3.00000000e+02
   3.00000000e+02 -0.00000000e+00 -1.00000000e+02 -1.00000000e+02
   6.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -2.45000000e+04  2.00000000e+02  5.00000000e+02 -0.00000000e+00
   6.00000000e+02 -2.00000000e+02  3.00000000e+02  2.00000000e+02
  -1.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -2.31000000e+04  1.20000002e+03  3.00000015e+02
   4.00000000e+02  4.99999977e+02  2.00000000e+02  1.39999998e+03
   2.00000019e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -2.26000000e+04  1.30000007e+03
  -0.00000000e+00  6.99999985e+02 -8.99999969e+02  1.00000000e+02
   1.59999995e+03]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -2.20000000e+04
   2.00000031e+02  2.99999969e+02 -5.00000000e+02  5.00000031e+02
  -5.00000031e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -2.47000000e+04  1.40000001e+03  1.10000000e+03  1.70000000e+03
  -7.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -2.35000000e+04  2.00000000e+02  5.00000000e+02
   1.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -2.43000000e+04  6.00000000e+02
   9.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -2.19000000e+04
   3.00000000e+02]]
tmp [[ 1.26000000e+04  3.60000000e+03  3.50000000e+03 -6.00000000e+02
   4.20000000e+03  8.00000000e+02  5.20000000e+03  2.20000000e+03
  -1.00000000e+03  3.20000000e+03 -1.00000000e+03  7.90000000e+03
  -1.20000000e+03  3.00000000e+03 -2.60000000e+03  2.80000000e+03
  -0.00000000e+00  4.20000000e+03 -7.00000000e+02 -2.60000000e+03
   4.60000000e+03  1.40000000e+03  1.70000000e+03  8.00000000e+02
  -1.40000000e+03]
 [ 0.00000000e+00  5.00000000e+02 -0.00000000e+00  4.00000000e+02
   8.00000031e+02 -1.52587900e-05  1.39999999e+03 -9.99999771e+01
  -1.20000000e+03 -1.00000015e+02  2.00000000e+02  8.00000000e+02
  -6.99999985e+02 -2.00000015e+02  4.99999985e+02  1.00000031e+02
   1.99999969e+02  1.52587882e-05  5.00000000e+02 -6.00000015e+02
   1.00000000e+03 -3.00000000e+02 -9.99999847e+01 -0.00000000e+00
   5.99999985e+02]
 [ 0.00000000e+00  0.00000000e+00  1.40000000e+03 -6.00000000e+02
   9.00000000e+02 -0.00000000e+00 -0.00000000e+00 -5.00000000e+02
   5.00000000e+02  5.00000000e+02 -1.00000000e+03  6.00000000e+02
  -1.00000000e+02 -2.00000000e+02  2.00000000e+02  1.00000000e+02
   4.00000000e+02 -1.00000000e+02 -6.00000000e+02  8.00000000e+02
  -3.00000000e+02 -0.00000000e+00  2.00000000e+02 -1.00000000e+02
   3.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  6.99999962e+02
  -9.99999619e+01  4.00000000e+02 -1.00000000e+03  8.99999962e+02
  -1.99999962e+02 -2.00000031e+02  7.99999992e+02 -6.00000000e+02
  -3.00000000e+02  3.00000000e+02  7.00000000e+02  3.00000000e+02
  -5.00000000e+02  5.00000000e+02 -4.00000000e+02 -1.00000000e+02
   1.00000000e+02  1.00000000e+02 -6.00000000e+02 -2.00000000e+02
   3.00000038e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000046e+02  1.00000000e+02  2.99999939e+02  6.00000000e+02
  -5.99999939e+02 -2.00000000e+02  9.99999390e+01  1.00000000e+02
   1.00000000e+02  8.00000000e+02 -9.00000000e+02  7.00000000e+02
  -4.00000000e+02 -2.99999939e+02  1.00000000e+02 -2.00000000e+02
   8.99999985e+02 -4.00000046e+02  6.00000000e+02 -9.99999542e+01
   3.99999954e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  1.70000003e+03  5.00000000e+02 -4.99999969e+02
   5.99999939e+02  5.00000031e+02 -2.00000000e+02 -1.00000000e+02
   9.99999847e+01  6.00000000e+02 -1.99999985e+02  1.99999985e+02
  -1.99999985e+02 -6.00000015e+02  2.00000000e+02  4.00000000e+02
   5.00000000e+02 -2.00000000e+02  1.09999998e+03 -8.99999985e+02
   2.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  1.10000000e+03  7.00000000e+02
   2.00000000e+02  3.00000000e+02 -5.00000000e+02  8.00000000e+02
   1.00000000e+02 -0.00000000e+00  1.30000000e+03 -5.00000000e+02
   1.00000000e+02 -9.00000000e+02 -0.00000000e+00 -0.00000000e+00
   1.20000000e+03 -1.10000000e+03  2.00000000e+02 -2.00000000e+02
   1.30000000e+03]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  2.10000000e+03
   1.30000000e+03 -8.00000000e+02  1.00000000e+03  1.70000000e+03
  -1.00000000e+03  5.00000000e+02  1.20000000e+03 -1.20000000e+03
   5.00000000e+02  2.00000000e+02  1.10000000e+03 -1.60000000e+03
   8.00000000e+02 -2.00000000e+02  5.00000000e+02 -7.00000000e+02
   4.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   3.00000000e+02  1.20000000e+03 -1.00000000e+02  3.00000000e+02
  -6.00000000e+02  1.10000000e+03 -5.00000000e+02  6.00000000e+02
  -6.00000000e+02 -4.00000000e+02  1.00000000e+02  3.00000000e+02
   5.00000000e+02 -8.00000000e+02 -0.00000000e+00  7.00000000e+02
  -1.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  1.10000002e+03  3.00000000e+02 -9.99999924e+01
   1.00000000e+02  9.99999847e+01 -2.99999992e+02  6.99999992e+02
  -2.00000000e+02 -1.20000003e+03 -1.99999939e+02  4.99999985e+02
  -3.00000046e+02  1.00000038e+02  3.00000023e+02  9.99999390e+01
  -0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  9.00000000e+02  2.00000000e+02
   5.00000000e+02 -2.00000000e+02  3.00000000e+02 -0.00000000e+00
   9.00000000e+02 -9.00000000e+02  4.00000000e+02  3.00000000e+02
   3.00000000e+02 -1.00000000e+02  2.00000000e+02 -4.00000000e+02
  -3.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  7.00000000e+02
   2.00000000e+02  3.00000000e+02 -4.00000000e+02 -2.00000000e+02
   3.00000000e+02 -2.00000000e+02 -0.00000000e+00  2.00000000e+02
   1.00000000e+02 -0.00000000e+00  4.00000000e+02 -7.00000000e+02
   5.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   9.99999969e+02 -0.00000000e+00  5.00000000e+02 -5.00000031e+02
   1.00000004e+03  4.99999977e+02 -7.00000000e+02  3.00000008e+02
   5.00000008e+02 -4.99999992e+02  8.99999989e+02 -5.00000000e+02
   1.00000000e+03]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  8.00000000e+02 -2.00000000e+02 -0.00000000e+00
   1.00000000e+02  3.00000000e+02 -1.00000015e+02 -0.00000000e+00
  -7.00000015e+02  3.00000000e+02  3.00000031e+02 -2.00000000e+02
   1.29999997e+03]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  1.20000000e+03  2.00000000e+02
  -4.00000000e+02 -0.00000000e+00 -0.00000000e+00  3.00000000e+02
   1.20000001e+03 -3.00000000e+02 -9.00000000e+02  6.00000000e+02
   1.00000000e+03]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  8.00000000e+02
   2.00000000e+02 -1.00000000e+02 -1.00000000e+02  7.00000000e+02
  -0.00000000e+00  1.00000000e+02  7.00000000e+02 -6.00000000e+02
   1.30000000e+03]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   2.00000000e+02 -2.00000000e+02  5.00000000e+02 -3.00000000e+02
   3.00000000e+02 -0.00000000e+00 -1.00000000e+02 -1.00000000e+02
   6.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  2.00000000e+02  5.00000000e+02 -0.00000000e+00
   6.00000000e+02 -2.00000000e+02  3.00000000e+02  2.00000000e+02
  -1.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  1.20000002e+03  3.00000015e+02
   4.00000000e+02  4.99999977e+02  2.00000000e+02  1.39999998e+03
   2.00000019e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.30000007e+03
  -0.00000000e+00  6.99999985e+02 -8.99999969e+02  1.00000000e+02
   1.59999995e+03]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   2.00000031e+02  2.99999969e+02 -5.00000000e+02  5.00000031e+02
  -5.00000031e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  1.40000001e+03  1.10000000e+03  1.70000000e+03
  -7.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  2.00000000e+02  5.00000000e+02
   1.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  6.00000000e+02
   9.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   3.00000000e+02]]
tmp [ 1.26000000e+04  4.10000000e+03  4.90000000e+03 -1.00000038e+02
  5.90000011e+03  3.00000002e+03  7.49999993e+03  5.40000002e+03
 -9.99999619e+01  5.50000001e+03  4.99999931e+02  1.23000000e+04
 -1.80000003e+03  6.89999997e+03  8.00000008e+02  4.09999998e+03
  1.60000002e+03  1.00000001e+03  1.80000006e+03  6.10351572e-05
  1.19000000e+04  7.99999939e+02  5.80000007e+03  2.50000001e+03
  9.19999989e+03]
cumsum_n_vids [1909 1979 2035 2108 2170 2262 2346 2430 2514 2592 2664 2732 2794 2864
 2947 3015 3083 3165 3243 3320 3395 3478 3558 3639 3713]
fgt [ 6.60031430e+00  2.07175341e+00  2.40786241e+00 -4.74383483e-02
  2.71889406e+00  1.32625995e+00  3.19693092e+00  2.22222223e+00
 -3.97772322e-02  2.12191358e+00  1.87687662e-01  4.50219620e+00
 -6.44237663e-01  2.40921787e+00  2.71462507e-01  1.35986732e+00
  5.18975032e-01  3.15955769e-01  5.55041647e-01  1.83840835e-08
  3.50515463e+00  2.30017234e-01  1.63012931e+00  6.87001926e-01
  2.47778074e+00]
fgt 1.623407419001824
fgt [1.62340742]
n_vids [1909   70   56   73   62   92   84   84   84   78   72   68   62   70
   83   68   68   82   78   77   75   83   80   81   74   70]
n_vids_pad [[1909.]
 [  70.]
 [  56.]
 [  73.]
 [  62.]
 [  92.]
 [  84.]
 [  84.]
 [  84.]
 [  78.]
 [  72.]
 [  68.]
 [  62.]
 [  70.]
 [  83.]
 [  68.]
 [  68.]
 [  82.]
 [  78.]
 [  77.]
 [  75.]
 [  83.]
 [  80.]
 [  81.]
 [  74.]
 [  70.]]
tmp [[157000.         151700.         151700.         148300.
  147200.         146300.         148000.         143400.
  141000.         142200.         138200.         139200.
  136900.         133300.         131600.         130400.
  132100.         132100.         128300.         127900.
  127600.         126600.         126000.         127800.
  126600.         128100.        ]
 [-14000.           4500.           4600.           4699.99996948
    4199.99998474   4399.99998474   4199.99998474   3899.99998474
    3899.99998474   3899.99998474   4499.99998474   4399.99998474
    4399.99996948   4399.99998474   4399.99998474   4199.99998474
    4099.99998474   4099.99996948   4499.99996948   3700.
    3900.           4599.99996948   4399.99998474   4499.99998474
    4899.99996948   4499.99998474]
 [-11200.         -11200.           4500.           3900.
    4000.           3900.           3300.           3500.
    3600.           3500.           3500.           3800.
    4000.           3900.           4200.           3700.
    3500.           3200.           3500.           3800.
    3800.           4000.           4200.           3500.
    3900.           3300.        ]
 [-14600.         -14600.         -14600.           5599.99993896
    5299.99993896   5199.99993896   4599.99997711   4999.99997711
    4799.99997711   4799.99997711   4599.99997711   4999.99997711
    4999.99997711   5299.99997711   5699.99997711   4699.99997711
    4499.99997711   4999.99997711   4799.99997711   5099.99997711
    4999.99997711   4999.99997711   4699.99997711   5299.99993896
    4599.99997711   5799.99996948]
 [-12400.         -12400.         -12400.         -12400.
    5800.00001526   5499.99996948   5800.00001526   4900.00003052
    5200.00003052   5400.00001526   5600.00001526   5400.00001526
    5199.99996948   5399.99996948   5400.00001526   5100.00001526
    5600.00001526   5400.00001526   4799.99996948   5099.99996948
    5499.99996948   5799.99995422   5599.99996948   5399.99996948
    5400.00001526   4899.99996948]
 [-18400.         -18400.         -18400.         -18400.
  -18400.           7899.99996948   7199.99996948   7699.99993896
    7799.99993896   7000.           6599.99996948   6300.
    6999.99993896   6699.99996948   7300.           7099.99993896
    6299.99998474   6099.99998474   6899.99996948   6700.
    6900.           6500.           6400.           6599.99998474
    6499.99996948   7000.        ]
 [-16800.         -16800.         -16800.         -16800.
  -16800.         -16800.           5800.           5400.
    5500.           5300.           5300.           5400.
    5100.           5200.           4900.           4300.
    4100.           3700.           4200.           4500.
    4100.           3300.           4100.           4300.
    3900.           4200.        ]
 [-16800.         -16800.         -16800.         -16800.
  -16800.         -16800.         -16800.           5300.
    4100.           3300.           3600.           3000.
    2900.           2700.           2800.           2200.
    2800.           2500.           2500.           2000.
    2700.           2500.           2600.           2300.
    2400.           2700.        ]
 [-16800.         -16800.         -16800.         -16800.
  -16800.         -16800.         -16800.         -16800.
    7300.           7000.           7300.           6800.
    7000.           7100.           6500.           6700.
    6800.           6700.           7100.           6300.
    7000.           6200.           5900.           6300.
    7100.           6000.        ]
 [-15600.         -15600.         -15600.         -15600.
  -15600.         -15600.         -15600.         -15600.
  -15600.           5700.           5699.99999237   5499.99999237
    5699.99999237   5700.           5500.           5600.
    5500.           5500.           6199.99999237   6099.99999237
    5700.           6099.99999237   5799.99999237   6299.99998474
    5499.99998474   5899.99999237]
 [-14400.         -14400.         -14400.         -14400.
  -14400.         -14400.         -14400.         -14400.
  -14400.         -14400.           5900.           5700.
    5200.           5400.           5000.           5000.
    5300.           4200.           5400.           4800.
    4600.           4300.           4800.           4800.
    5900.           5600.        ]
 [-13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.           5800.
    5600.           5700.           5800.           5700.
    6000.           5600.           6300.           5900.
    5500.           5300.           5300.           5300.
    5600.           5600.        ]
 [-12400.         -12400.         -12400.         -12400.
  -12400.         -12400.         -12400.         -12400.
  -12400.         -12400.         -12400.         -12400.
    5100.00003052   4799.99998474   4900.00003052   3400.00000763
    3599.99998474   3400.           2900.00001526   3499.99999237
    3100.           2700.           2900.00000763   3000.00000763
    3300.           3499.99999237]
 [-14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.           5899.99996948   5499.99996948   5399.99998474
    5599.99996948   5599.99996948   4799.99998474   5199.99998474
    5199.99998474   5400.           5700.           5800.
    5199.99996948   5199.99996948]
 [-16600.         -16600.         -16600.         -16600.
  -16600.         -16600.         -16600.         -16600.
  -16600.         -16600.         -16600.         -16600.
  -16600.         -16600.           6200.           5900.
    6100.           6600.           6000.           5899.99999237
    5599.99999237   4599.99999237   5099.99999237   5499.99999237
    5099.99999237   4699.99999237]
 [-13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.           6100.
    5600.           5000.           5100.           4700.
    4400.           4800.           4700.           5000.
    4300.           4500.        ]
 [-13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.         -13600.
  -13600.         -13600.         -13600.         -13600.
    5700.           5300.           5300.           5600.
    5900.           5700.           5700.           6000.
    5800.           5900.        ]
 [-16400.         -16400.         -16400.         -16400.
  -16400.         -16400.         -16400.         -16400.
  -16400.         -16400.         -16400.         -16400.
  -16400.         -16400.         -16400.         -16400.
  -16400.           7600.           7400.           7300.
    7100.           6400.           6900.           6800.
    6300.           6600.        ]
 [-15600.         -15600.         -15600.         -15600.
  -15600.         -15600.         -15600.         -15600.
  -15600.         -15600.         -15600.         -15600.
  -15600.         -15600.         -15600.         -15600.
  -15600.         -15600.           5800.00001526   4999.99999237
    5500.00003052   4699.99999237   4799.99999237   5199.99998474
    4499.99999237   4800.00003052]
 [-15400.         -15400.         -15400.         -15400.
  -15400.         -15400.         -15400.         -15400.
  -15400.         -15400.         -15400.         -15400.
  -15400.         -15400.         -15400.         -15400.
  -15400.         -15400.         -15400.           6600.00001526
    5999.99993134   5699.99994659   5699.99994659   5599.99994659
    5699.99994659   4999.99994659]
 [-15000.         -15000.         -15000.         -15000.
  -15000.         -15000.         -15000.         -15000.
  -15000.         -15000.         -15000.         -15000.
  -15000.         -15000.         -15000.         -15000.
  -15000.         -15000.         -15000.         -15000.
    6600.           6399.99996948   6299.99996948   6599.99996948
    6099.99996948   6500.        ]
 [-16600.         -16600.         -16600.         -16600.
  -16600.         -16600.         -16600.         -16600.
  -16600.         -16600.         -16600.         -16600.
  -16600.         -16600.         -16600.         -16600.
  -16600.         -16600.         -16600.         -16600.
  -16600.           7700.           4899.99999619   4999.99999619
    4899.99999619   4899.99999619]
 [-16000.         -16000.         -16000.         -16000.
  -16000.         -16000.         -16000.         -16000.
  -16000.         -16000.         -16000.         -16000.
  -16000.         -16000.         -16000.         -16000.
  -16000.         -16000.         -16000.         -16000.
  -16000.         -16000.           7000.           6900.
    6800.           6700.        ]
 [-16200.         -16200.         -16200.         -16200.
  -16200.         -16200.         -16200.         -16200.
  -16200.         -16200.         -16200.         -16200.
  -16200.         -16200.         -16200.         -16200.
  -16200.         -16200.         -16200.         -16200.
  -16200.         -16200.         -16200.           7000.
    6400.           5700.        ]
 [-14800.         -14800.         -14800.         -14800.
  -14800.         -14800.         -14800.         -14800.
  -14800.         -14800.         -14800.         -14800.
  -14800.         -14800.         -14800.         -14800.
  -14800.         -14800.         -14800.         -14800.
  -14800.         -14800.         -14800.         -14800.
    6200.           6100.        ]
 [-14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.         -14000.         -14000.         -14000.
  -14000.           4300.        ]]
tmp [[ 5.30000000e+03 -0.00000000e+00  3.40000000e+03  1.10000000e+03
   9.00000000e+02 -1.70000000e+03  4.60000000e+03  2.40000000e+03
  -1.20000000e+03  4.00000000e+03 -1.00000000e+03  2.30000000e+03
   3.60000000e+03  1.70000000e+03  1.20000000e+03 -1.70000000e+03
  -0.00000000e+00  3.80000000e+03  4.00000000e+02  3.00000000e+02
   1.00000000e+03  6.00000000e+02 -1.80000000e+03  1.20000000e+03
  -1.50000000e+03]
 [-1.85000000e+04 -1.00000000e+02 -9.99999695e+01  4.99999985e+02
  -2.00000000e+02  2.00000000e+02  3.00000000e+02 -0.00000000e+00
  -0.00000000e+00 -6.00000000e+02  1.00000000e+02  1.52587891e-05
  -1.52587891e-05 -0.00000000e+00  2.00000000e+02  1.00000000e+02
   1.52587891e-05 -4.00000000e+02  7.99999969e+02 -2.00000000e+02
  -6.99999969e+02  1.99999985e+02 -1.00000000e+02 -3.99999985e+02
   3.99999985e+02]
 [-0.00000000e+00 -1.57000000e+04  6.00000000e+02 -1.00000000e+02
   1.00000000e+02  6.00000000e+02 -2.00000000e+02 -1.00000000e+02
   1.00000000e+02 -0.00000000e+00 -3.00000000e+02 -2.00000000e+02
   1.00000000e+02 -3.00000000e+02  5.00000000e+02  2.00000000e+02
   3.00000000e+02 -3.00000000e+02 -3.00000000e+02 -0.00000000e+00
  -2.00000000e+02 -2.00000000e+02  7.00000000e+02 -4.00000000e+02
   6.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -2.01999999e+04  3.00000000e+02
   1.00000000e+02  5.99999962e+02 -4.00000000e+02  2.00000000e+02
  -0.00000000e+00  2.00000000e+02 -4.00000000e+02 -0.00000000e+00
  -3.00000000e+02 -4.00000000e+02  1.00000000e+03  2.00000000e+02
  -5.00000000e+02  2.00000000e+02 -3.00000000e+02  1.00000000e+02
  -0.00000000e+00  3.00000000e+02 -5.99999962e+02  6.99999962e+02
  -1.19999999e+03]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -1.82000000e+04
   3.00000046e+02 -3.00000046e+02  8.99999985e+02 -3.00000000e+02
  -1.99999985e+02 -2.00000000e+02  2.00000000e+02  2.00000046e+02
  -2.00000000e+02 -4.57763672e-05  3.00000000e+02 -5.00000000e+02
   2.00000000e+02  6.00000046e+02 -3.00000000e+02 -4.00000000e+02
  -2.99999985e+02  1.99999985e+02  2.00000000e+02 -4.57763672e-05
   5.00000046e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -2.63000000e+04  7.00000000e+02 -4.99999969e+02 -1.00000000e+02
   7.99999939e+02  4.00000031e+02  2.99999969e+02 -6.99999939e+02
   2.99999969e+02 -6.00000031e+02  2.00000061e+02  7.99999954e+02
   2.00000000e+02 -7.99999985e+02  1.99999969e+02 -2.00000000e+02
   4.00000000e+02  1.00000000e+02 -1.99999985e+02  1.00000015e+02
  -5.00000031e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -2.26000000e+04  4.00000000e+02 -1.00000000e+02
   2.00000000e+02 -0.00000000e+00 -1.00000000e+02  3.00000000e+02
  -1.00000000e+02  3.00000000e+02  6.00000000e+02  2.00000000e+02
   4.00000000e+02 -5.00000000e+02 -3.00000000e+02  4.00000000e+02
   8.00000000e+02 -8.00000000e+02 -2.00000000e+02  4.00000000e+02
  -3.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -2.21000000e+04  1.20000000e+03
   8.00000000e+02 -3.00000000e+02  6.00000000e+02  1.00000000e+02
   2.00000000e+02 -1.00000000e+02  6.00000000e+02 -6.00000000e+02
   3.00000000e+02 -0.00000000e+00  5.00000000e+02 -7.00000000e+02
   2.00000000e+02 -1.00000000e+02  3.00000000e+02 -1.00000000e+02
  -3.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -2.41000000e+04
   3.00000000e+02 -3.00000000e+02  5.00000000e+02 -2.00000000e+02
  -1.00000000e+02  6.00000000e+02 -2.00000000e+02 -1.00000000e+02
   1.00000000e+02 -4.00000000e+02  8.00000000e+02 -7.00000000e+02
   8.00000000e+02  3.00000000e+02 -4.00000000e+02 -8.00000000e+02
   1.10000000e+03]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -2.13000000e+04  7.62939453e-06  2.00000000e+02 -2.00000000e+02
  -7.62939453e-06  2.00000000e+02 -1.00000000e+02  1.00000000e+02
  -0.00000000e+00 -6.99999992e+02  1.00000000e+02  3.99999992e+02
  -3.99999992e+02  3.00000000e+02 -4.99999992e+02  8.00000000e+02
  -4.00000008e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -2.03000000e+04  2.00000000e+02  5.00000000e+02
  -2.00000000e+02  4.00000000e+02 -0.00000000e+00 -3.00000000e+02
   1.10000000e+03 -1.20000000e+03  6.00000000e+02  2.00000000e+02
   3.00000000e+02 -5.00000000e+02 -0.00000000e+00 -1.10000000e+03
   3.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -1.94000000e+04  2.00000000e+02
  -1.00000000e+02 -1.00000000e+02  1.00000000e+02 -3.00000000e+02
   4.00000000e+02 -7.00000000e+02  4.00000000e+02  4.00000000e+02
   2.00000000e+02 -0.00000000e+00 -0.00000000e+00 -3.00000000e+02
  -0.00000000e+00]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -1.75000000e+04
   3.00000046e+02 -1.00000046e+02  1.50000002e+03 -1.99999977e+02
   1.99999985e+02  4.99999985e+02 -5.99999977e+02  3.99999992e+02
   4.00000000e+02 -2.00000008e+02 -1.00000000e+02 -2.99999992e+02
  -1.99999992e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -1.99000000e+04  4.00000000e+02  9.99999847e+01 -1.99999985e+02
  -0.00000000e+00  7.99999985e+02 -4.00000000e+02 -0.00000000e+00
  -2.00000015e+02 -3.00000000e+02 -1.00000000e+02  6.00000031e+02
  -0.00000000e+00]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -2.28000000e+04  3.00000000e+02 -2.00000000e+02
  -5.00000000e+02  6.00000000e+02  1.00000008e+02  3.00000000e+02
   1.00000000e+03 -5.00000000e+02 -4.00000000e+02  4.00000000e+02
   4.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -1.97000000e+04  5.00000000e+02
   6.00000000e+02 -1.00000000e+02  4.00000000e+02  3.00000000e+02
  -4.00000000e+02  1.00000000e+02 -3.00000000e+02  7.00000000e+02
  -2.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -1.93000000e+04
   4.00000000e+02 -0.00000000e+00 -3.00000000e+02 -3.00000000e+02
   2.00000000e+02 -0.00000000e+00 -3.00000000e+02  2.00000000e+02
  -1.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -2.40000000e+04  2.00000000e+02  1.00000000e+02  2.00000000e+02
   7.00000000e+02 -5.00000000e+02  1.00000000e+02  5.00000000e+02
  -3.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -2.14000000e+04  8.00000023e+02 -5.00000038e+02
   8.00000038e+02 -1.00000000e+02 -3.99999992e+02  6.99999992e+02
  -3.00000038e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -2.20000000e+04  6.00000084e+02
   2.99999985e+02 -0.00000000e+00  1.00000000e+02 -1.00000000e+02
   7.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -2.16000000e+04
   2.00000031e+02  1.00000000e+02 -3.00000000e+02  5.00000000e+02
  -4.00000031e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -2.43000000e+04  2.80000000e+03 -1.00000000e+02  1.00000000e+02
  -0.00000000e+00]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -2.30000000e+04  1.00000000e+02  1.00000000e+02
   1.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -2.32000000e+04  6.00000000e+02
   7.00000000e+02]
 [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00
  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -2.10000000e+04
   1.00000000e+02]]
tmp [[ 5.30000000e+03 -0.00000000e+00  3.40000000e+03  1.10000000e+03
   9.00000000e+02 -1.70000000e+03  4.60000000e+03  2.40000000e+03
  -1.20000000e+03  4.00000000e+03 -1.00000000e+03  2.30000000e+03
   3.60000000e+03  1.70000000e+03  1.20000000e+03 -1.70000000e+03
  -0.00000000e+00  3.80000000e+03  4.00000000e+02  3.00000000e+02
   1.00000000e+03  6.00000000e+02 -1.80000000e+03  1.20000000e+03
  -1.50000000e+03]
 [ 0.00000000e+00 -1.00000000e+02 -9.99999695e+01  4.99999985e+02
  -2.00000000e+02  2.00000000e+02  3.00000000e+02 -0.00000000e+00
  -0.00000000e+00 -6.00000000e+02  1.00000000e+02  1.52587891e-05
  -1.52587891e-05 -0.00000000e+00  2.00000000e+02  1.00000000e+02
   1.52587891e-05 -4.00000000e+02  7.99999969e+02 -2.00000000e+02
  -6.99999969e+02  1.99999985e+02 -1.00000000e+02 -3.99999985e+02
   3.99999985e+02]
 [ 0.00000000e+00  0.00000000e+00  6.00000000e+02 -1.00000000e+02
   1.00000000e+02  6.00000000e+02 -2.00000000e+02 -1.00000000e+02
   1.00000000e+02 -0.00000000e+00 -3.00000000e+02 -2.00000000e+02
   1.00000000e+02 -3.00000000e+02  5.00000000e+02  2.00000000e+02
   3.00000000e+02 -3.00000000e+02 -3.00000000e+02 -0.00000000e+00
  -2.00000000e+02 -2.00000000e+02  7.00000000e+02 -4.00000000e+02
   6.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  3.00000000e+02
   1.00000000e+02  5.99999962e+02 -4.00000000e+02  2.00000000e+02
  -0.00000000e+00  2.00000000e+02 -4.00000000e+02 -0.00000000e+00
  -3.00000000e+02 -4.00000000e+02  1.00000000e+03  2.00000000e+02
  -5.00000000e+02  2.00000000e+02 -3.00000000e+02  1.00000000e+02
  -0.00000000e+00  3.00000000e+02 -5.99999962e+02  6.99999962e+02
  -1.19999999e+03]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   3.00000046e+02 -3.00000046e+02  8.99999985e+02 -3.00000000e+02
  -1.99999985e+02 -2.00000000e+02  2.00000000e+02  2.00000046e+02
  -2.00000000e+02 -4.57763672e-05  3.00000000e+02 -5.00000000e+02
   2.00000000e+02  6.00000046e+02 -3.00000000e+02 -4.00000000e+02
  -2.99999985e+02  1.99999985e+02  2.00000000e+02 -4.57763672e-05
   5.00000046e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  7.00000000e+02 -4.99999969e+02 -1.00000000e+02
   7.99999939e+02  4.00000031e+02  2.99999969e+02 -6.99999939e+02
   2.99999969e+02 -6.00000031e+02  2.00000061e+02  7.99999954e+02
   2.00000000e+02 -7.99999985e+02  1.99999969e+02 -2.00000000e+02
   4.00000000e+02  1.00000000e+02 -1.99999985e+02  1.00000015e+02
  -5.00000031e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  4.00000000e+02 -1.00000000e+02
   2.00000000e+02 -0.00000000e+00 -1.00000000e+02  3.00000000e+02
  -1.00000000e+02  3.00000000e+02  6.00000000e+02  2.00000000e+02
   4.00000000e+02 -5.00000000e+02 -3.00000000e+02  4.00000000e+02
   8.00000000e+02 -8.00000000e+02 -2.00000000e+02  4.00000000e+02
  -3.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.20000000e+03
   8.00000000e+02 -3.00000000e+02  6.00000000e+02  1.00000000e+02
   2.00000000e+02 -1.00000000e+02  6.00000000e+02 -6.00000000e+02
   3.00000000e+02 -0.00000000e+00  5.00000000e+02 -7.00000000e+02
   2.00000000e+02 -1.00000000e+02  3.00000000e+02 -1.00000000e+02
  -3.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   3.00000000e+02 -3.00000000e+02  5.00000000e+02 -2.00000000e+02
  -1.00000000e+02  6.00000000e+02 -2.00000000e+02 -1.00000000e+02
   1.00000000e+02 -4.00000000e+02  8.00000000e+02 -7.00000000e+02
   8.00000000e+02  3.00000000e+02 -4.00000000e+02 -8.00000000e+02
   1.10000000e+03]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  7.62939453e-06  2.00000000e+02 -2.00000000e+02
  -7.62939453e-06  2.00000000e+02 -1.00000000e+02  1.00000000e+02
  -0.00000000e+00 -6.99999992e+02  1.00000000e+02  3.99999992e+02
  -3.99999992e+02  3.00000000e+02 -4.99999992e+02  8.00000000e+02
  -4.00000008e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  2.00000000e+02  5.00000000e+02
  -2.00000000e+02  4.00000000e+02 -0.00000000e+00 -3.00000000e+02
   1.10000000e+03 -1.20000000e+03  6.00000000e+02  2.00000000e+02
   3.00000000e+02 -5.00000000e+02 -0.00000000e+00 -1.10000000e+03
   3.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  2.00000000e+02
  -1.00000000e+02 -1.00000000e+02  1.00000000e+02 -3.00000000e+02
   4.00000000e+02 -7.00000000e+02  4.00000000e+02  4.00000000e+02
   2.00000000e+02 -0.00000000e+00 -0.00000000e+00 -3.00000000e+02
  -0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   3.00000046e+02 -1.00000046e+02  1.50000002e+03 -1.99999977e+02
   1.99999985e+02  4.99999985e+02 -5.99999977e+02  3.99999992e+02
   4.00000000e+02 -2.00000008e+02 -1.00000000e+02 -2.99999992e+02
  -1.99999992e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  4.00000000e+02  9.99999847e+01 -1.99999985e+02
  -0.00000000e+00  7.99999985e+02 -4.00000000e+02 -0.00000000e+00
  -2.00000015e+02 -3.00000000e+02 -1.00000000e+02  6.00000031e+02
  -0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  3.00000000e+02 -2.00000000e+02
  -5.00000000e+02  6.00000000e+02  1.00000008e+02  3.00000000e+02
   1.00000000e+03 -5.00000000e+02 -4.00000000e+02  4.00000000e+02
   4.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  5.00000000e+02
   6.00000000e+02 -1.00000000e+02  4.00000000e+02  3.00000000e+02
  -4.00000000e+02  1.00000000e+02 -3.00000000e+02  7.00000000e+02
  -2.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   4.00000000e+02 -0.00000000e+00 -3.00000000e+02 -3.00000000e+02
   2.00000000e+02 -0.00000000e+00 -3.00000000e+02  2.00000000e+02
  -1.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  2.00000000e+02  1.00000000e+02  2.00000000e+02
   7.00000000e+02 -5.00000000e+02  1.00000000e+02  5.00000000e+02
  -3.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  8.00000023e+02 -5.00000038e+02
   8.00000038e+02 -1.00000000e+02 -3.99999992e+02  6.99999992e+02
  -3.00000038e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  6.00000084e+02
   2.99999985e+02 -0.00000000e+00  1.00000000e+02 -1.00000000e+02
   7.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   2.00000031e+02  1.00000000e+02 -3.00000000e+02  5.00000000e+02
  -4.00000031e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  2.80000000e+03 -1.00000000e+02  1.00000000e+02
  -0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  1.00000000e+02  1.00000000e+02
   1.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  6.00000000e+02
   7.00000000e+02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+02]]
tmp [ 5300.          -100.          3900.00003052  1799.99998474
  1200.00004578    99.99991608  5100.00001526  3200.
   799.99995422  3200.00003815   299.99996948  2300.00012207
  3499.99999237  1999.99987793  6300.00006866 -2000.00000763
  3200.          1600.00003815  2699.99999237   600.00003052
  5100.00009155  1799.99996567 -4299.99993134  4099.99997711
  -800.00006104]
cumsum_n_vids [1909 1979 2035 2108 2170 2262 2346 2430 2514 2592 2664 2732 2794 2864
 2947 3015 3083 3165 3243 3320 3395 3478 3558 3639 3713]
fgt [ 2.77632268 -0.05053057  1.91646193  0.85388994  0.55299541  0.04420863
  2.17391305  1.31687243  0.31821796  1.23456792  0.1126126   0.84187413
  1.25268432  0.69832398  2.13776724 -0.66334992  1.03795005  0.50552924
  0.83256244  0.1807229   1.50220916  0.51753881 -1.20854411  1.12668315
 -0.21545921]
fgt 0.7918409659434856
fgt [0.79184097]