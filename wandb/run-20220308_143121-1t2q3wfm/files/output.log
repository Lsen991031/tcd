ucf101: 101 classes
Method : OURS
----AGE 0----
current_task  [37, 97, 56, 55, 33, 84, 3, 4, 72, 59, 66, 48, 65, 91, 99, 39, 34, 22, 67, 74, 19, 35, 9, 86, 88, 63, 85, 38, 54, 25, 57, 62, 83, 76, 6, 13, 2, 53, 8, 24, 44, 12, 100, 29, 5, 17, 15, 73, 47, 27, 46]
current_head  51
Phase 2 : Train RGB Model in an Incremental Manner
=> base model: resnet34
CosineLinear(input_features=512, output_features=153, sigma=tensor([1.]), eta=tensor([1.]))
video number : 4793
video number + exemplar : 4793
DataLoader Constructed : Train 149
Optimizer Constructed
/home/ustc/anaconda3/envs/lhc/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2022-03-08 14:31:44.473342
Epoch: [0][0/149], lr: 0.00100	Time 13.661 (13.661)	Data 2.540 (2.540)	Loss 3.9903 (3.9903)	Loss CE 3.9284 (3.9284)	Loss KD (Logit) 0.0000 (0.0000)	Loss KD (GCAM) 0.0000 (0.0000)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6193 (0.6193)	Loss REG 0.0000 (0.0000)	Prec@1 3.125 (3.125)
2022-03-08 14:32:43.561492
Epoch: [0][100/149], lr: 0.00100	Time 0.390 (0.720)	Data 0.000 (0.025)	Loss 3.9829 (3.9906)	Loss CE 3.9226 (3.9284)	Loss KD (Logit) 0.0000 (0.0000)	Loss KD (GCAM) 0.0000 (0.0000)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6026 (0.6220)	Loss REG 0.0000 (0.0000)	Prec@1 0.000 (2.847)
Sigma : Parameter containing:
tensor([1.0671], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0317], device='cuda:0', requires_grad=True)
Update Importance Mask...
Phase 3 : Manage Exemplar Sets
=> base model: resnet34
Construct Exemplar Set
Load the Model
CosineLinear(input_features=512, output_features=153, sigma=tensor([1.0671]), eta=tensor([1.0317]))
Exemplar per class : 5
video number : 4793
video number + exemplar : 4793
Phase 4 : Class-balanced Fine-Tuning : SKIP
Phase 5 : Eval RGB Model for the Tasks Trained so far
=> base model: resnet34
Load the Trained Model from checkpoint/ucf101/51/2/000/task_000.pth.tar
exemplar : 255
Computing the class mean vectors...
Eval Task 0 for Age 0
Current Task : [37, 97, 56, 55, 33, 84, 3, 4, 72, 59, 66, 48, 65, 91, 99, 39, 34, 22, 67, 74, 19, 35, 9, 86, 88, 63, 85, 38, 54, 25, 57, 62, 83, 76, 6, 13, 2, 53, 8, 24, 44, 12, 100, 29, 5, 17, 15, 73, 47, 27, 46]
video number : 1909
video number + exemplar : 1909
DataLoader Constructed
Test: [0/120]	Time 8.278 (8.278)	Prec@1 12.500 (12.500)
Test: [100/120]	Time 0.507 (0.714)	Prec@1 12.500 (7.054)
Testing Results: Prec@1 7.019
Classify using the NME Classifier...
Test (NME): [0/120]	Time 0.000 (0.000)	Prec@1 37.500 (37.500)
Test (NME): [100/120]	Time 0.000 (0.000)	Prec@1 62.500 (50.248)
Testing Results (NME): Prec@1 50.236
Method : OURS
----AGE 1----
current_task  [98, 96]
current_head  53
Phase 2 : Train RGB Model in an Incremental Manner
=> base model: resnet34
Load the Previous Model
Copy the old Model
lambda_0  : [1.0, 0.05049752469181039]
Increment the Model
SplitCosineLinear(
  input_features=512, output_features=159, sigma=tensor([1.0671]), eta=tensor([1.0317])
  (fc1): CosineLinear(input_features=512, output_features=153, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
video number : 176
video number + exemplar : 431
DataLoader Constructed : Train 13
Optimizer Constructed
video number : 176
video number + exemplar : 176
Initialize Cosine Classifier
Computing the class mean vectors...
/home/ustc/anaconda3/envs/lhc/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2022-03-08 14:38:14.923762
Epoch: [0][0/13], lr: 0.00100	Time 3.675 (3.675)	Data 1.827 (1.827)	Loss 3.8448 (3.8448)	Loss CE 3.7597 (3.7597)	Loss KD (Logit) 0.2790 (0.2790)	Loss KD (GCAM) 0.0334 (0.0334)	Loss MR 0.0000 (0.0000)	Loss DIV 0.6100 (0.6100)	Loss REG 0.0000 (0.0000)	Prec@1 43.750 (43.750)
Sigma : Parameter containing:
tensor([1.1987], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.1001], device='cuda:0', requires_grad=True)
Update Importance Mask...
Phase 3 : Manage Exemplar Sets
=> base model: resnet34
Construct Exemplar Set
Load the Model
SplitCosineLinear(
  input_features=512, output_features=159, sigma=tensor([1.1987]), eta=tensor([1.1001])
  (fc1): CosineLinear(input_features=512, output_features=153, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
Exemplar per class : 5
video number : 176
video number + exemplar : 176
Phase 4 : Class-balanced Fine-Tuning
=> base model: resnet34
Load the Model
SplitCosineLinear(
  input_features=512, output_features=159, sigma=tensor([1.1987]), eta=tensor([1.1001])
  (fc1): CosineLinear(input_features=512, output_features=153, sigma=1.0, eta=1.0)
  (fc2): CosineLinear(input_features=512, output_features=6, sigma=1.0, eta=1.0)
)
exemplar : 265
DataLoader CBF Constructed : Train 8
Optimizer Constructed
2022-03-08 14:38:41.619911
Epoch: [0][0/8], lr: 0.00050	Time 2.487 (2.487)	Data 1.646 (1.646)	Loss 4.0173 (4.0173)	Prec@1 0.000 (0.000)	Prec@5 9.375 (9.375)
Sigma : Parameter containing:
tensor([1.1936], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0971], device='cuda:0', requires_grad=True)
2022-03-08 14:38:48.733368
Epoch: [1][0/8], lr: 0.00050	Time 2.776 (2.776)	Data 2.161 (2.161)	Loss 3.9685 (3.9685)	Prec@1 3.125 (3.125)	Prec@5 21.875 (21.875)
Sigma : Parameter containing:
tensor([1.1822], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0907], device='cuda:0', requires_grad=True)
2022-03-08 14:38:55.092574
Epoch: [2][0/8], lr: 0.00050	Time 2.883 (2.883)	Data 2.233 (2.233)	Loss 4.0031 (4.0031)	Prec@1 0.000 (0.000)	Prec@5 15.625 (15.625)
Sigma : Parameter containing:
tensor([1.1707], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0841], device='cuda:0', requires_grad=True)
2022-03-08 14:39:01.285898
Epoch: [3][0/8], lr: 0.00050	Time 2.650 (2.650)	Data 2.001 (2.001)	Loss 4.0084 (4.0084)	Prec@1 0.000 (0.000)	Prec@5 9.375 (9.375)
Sigma : Parameter containing:
tensor([1.1614], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0788], device='cuda:0', requires_grad=True)
2022-03-08 14:39:07.322363
Epoch: [4][0/8], lr: 0.00050	Time 2.789 (2.789)	Data 2.068 (2.068)	Loss 3.9605 (3.9605)	Prec@1 3.125 (3.125)	Prec@5 18.750 (18.750)
Sigma : Parameter containing:
tensor([1.1538], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0744], device='cuda:0', requires_grad=True)
2022-03-08 14:39:13.982996
Epoch: [5][0/8], lr: 0.00050	Time 3.135 (3.135)	Data 1.972 (1.972)	Loss 3.9953 (3.9953)	Prec@1 0.000 (0.000)	Prec@5 25.000 (25.000)
Sigma : Parameter containing:
tensor([1.1479], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0708], device='cuda:0', requires_grad=True)
2022-03-08 14:39:22.973870
Epoch: [6][0/8], lr: 0.00050	Time 3.191 (3.191)	Data 2.265 (2.265)	Loss 3.9764 (3.9764)	Prec@1 3.125 (3.125)	Prec@5 12.500 (12.500)
Sigma : Parameter containing:
tensor([1.1435], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0682], device='cuda:0', requires_grad=True)
2022-03-08 14:39:31.696671
Epoch: [7][0/8], lr: 0.00050	Time 2.927 (2.927)	Data 1.973 (1.973)	Loss 3.9892 (3.9892)	Prec@1 0.000 (0.000)	Prec@5 18.750 (18.750)
Sigma : Parameter containing:
tensor([1.1403], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0662], device='cuda:0', requires_grad=True)
2022-03-08 14:39:40.627932
Epoch: [8][0/8], lr: 0.00050	Time 3.132 (3.132)	Data 2.210 (2.210)	Loss 3.9381 (3.9381)	Prec@1 6.250 (6.250)	Prec@5 21.875 (21.875)
Sigma : Parameter containing:
tensor([1.1392], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0653], device='cuda:0', requires_grad=True)
2022-03-08 14:39:49.469276
Epoch: [9][0/8], lr: 0.00050	Time 3.022 (3.022)	Data 1.920 (1.920)	Loss 3.9536 (3.9536)	Prec@1 3.125 (3.125)	Prec@5 34.375 (34.375)
Sigma : Parameter containing:
tensor([1.1379], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0644], device='cuda:0', requires_grad=True)
2022-03-08 14:39:58.405228
Epoch: [10][0/8], lr: 0.00050	Time 3.053 (3.053)	Data 2.162 (2.162)	Loss 3.9811 (3.9811)	Prec@1 0.000 (0.000)	Prec@5 18.750 (18.750)
Sigma : Parameter containing:
tensor([1.1369], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0635], device='cuda:0', requires_grad=True)
2022-03-08 14:40:07.179163
Epoch: [11][0/8], lr: 0.00050	Time 2.988 (2.988)	Data 1.981 (1.981)	Loss 3.9135 (3.9135)	Prec@1 9.375 (9.375)	Prec@5 31.250 (31.250)
Sigma : Parameter containing:
tensor([1.1389], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0642], device='cuda:0', requires_grad=True)
2022-03-08 14:40:16.112185
Epoch: [12][0/8], lr: 0.00050	Time 3.118 (3.118)	Data 2.423 (2.423)	Loss 3.9583 (3.9583)	Prec@1 3.125 (3.125)	Prec@5 12.500 (12.500)
Sigma : Parameter containing:
tensor([1.1400], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0644], device='cuda:0', requires_grad=True)
2022-03-08 14:40:25.133830
Epoch: [13][0/8], lr: 0.00050	Time 3.176 (3.176)	Data 2.083 (2.083)	Loss 3.9393 (3.9393)	Prec@1 6.250 (6.250)	Prec@5 25.000 (25.000)
Sigma : Parameter containing:
tensor([1.1458], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0672], device='cuda:0', requires_grad=True)
2022-03-08 14:40:32.448269
Epoch: [14][0/8], lr: 0.00050	Time 2.801 (2.801)	Data 2.025 (2.025)	Loss 3.9723 (3.9723)	Prec@1 0.000 (0.000)	Prec@5 31.250 (31.250)
Sigma : Parameter containing:
tensor([1.1497], device='cuda:0', requires_grad=True), Eta : Parameter containing:
tensor([1.0691], device='cuda:0', requires_grad=True)
2022-03-08 14:40:39.591592
Epoch: [15][0/8], lr: 0.00050	Time 2.881 (2.881)	Data 2.013 (2.013)	Loss 3.9059 (3.9059)	Prec@1 9.375 (9.375)	Prec@5 28.125 (28.125)
Traceback (most recent call last):
  File "main.py", line 101, in <module>
    main()
  File "main.py", line 82, in main
    cbf.train_task(args, i, total_task_list[:i+1], current_head, class_indexer, prefix)
  File "/home/ustc/ls/tcd_code/train/cbf.py", line 241, in train_task
    _train(args, train_loader, model, criterion, optimizer, epoch, age)#, lambda_0=lambda_0, model_old=model_old)
  File "/home/ustc/ls/tcd_code/train/cbf.py", line 83, in _train
    total_norm = clip_grad_norm_(model.parameters(), args.clip_gradient)
  File "/home/ustc/anaconda3/envs/lhc/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py", line 33, in clip_grad_norm_
    total_norm += param_norm.item() ** norm_type
KeyboardInterrupt